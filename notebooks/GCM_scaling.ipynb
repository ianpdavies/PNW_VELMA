{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling historical climate normals to 2100 using GCM projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import __init__\n",
    "import scripts.config as config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import datetime\n",
    "from sklearn.svm import SVR\n",
    "from natsort import natsorted\n",
    "import geopandas as gpd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import HydroErr as he\n",
    "import os\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting parameters\n",
    "\n",
    "XSMALL_SIZE = 6\n",
    "SMALL_SIZE = 7\n",
    "MEDIUM_SIZE = 9\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams['figure.dpi'] = 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\anaconda3\\envs\\tnc_velma\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3147: DtypeWarning: Columns (9,13,37,43,45) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Average PRISM and Naselle gauge precipitation\n",
    "gauge = pd.read_csv(config.daily_ppt.parents[0] / 'GHCND_USC00455774_1929_2020.csv', parse_dates=True, index_col=5)\n",
    "gauge['SNOW'].fillna(0, inplace=True)\n",
    "gauge['SNOW_SWE'] = gauge['SNOW'] / 13\n",
    "gauge['PRCP_TOT'] = gauge['PRCP'] + gauge['SNOW_SWE']\n",
    "gauge_precip = gauge[['PRCP_TOT']]\n",
    "\n",
    "prism_precip = pd.read_csv(str(config.daily_ppt.parents[0] / 'prism_ppt_1981_2020_daily.csv'), parse_dates=True, index_col=0)\n",
    "\n",
    "# Expand precip record to full date range in case some days are missing\n",
    "hist_start = pd.to_datetime('01-01-1981')\n",
    "hist_end = pd.to_datetime('12-31-2020')\n",
    "rng = pd.date_range(hist_start, hist_end)\n",
    "date_df = pd.DataFrame(index=rng)\n",
    "gauge_precip_velma = date_df.merge(gauge_precip, left_index=True, right_index=True, how='left')\n",
    "prism_precip_velma = date_df.merge(prism_precip, left_index=True, right_index=True, how='left')\n",
    "\n",
    "prism_precip_mean = pd.concat([gauge_precip_velma, prism_precip_velma], axis=1)\n",
    "prism_precip_mean['avg_precip'] = prism_precip_mean.mean(axis=1)\n",
    "\n",
    "# Air temperature\n",
    "temp_path = config.daily_temp_mean.parents[0] / 'prism_temp_1981_2020_daily.csv'\n",
    "temp = pd.read_csv(temp_path, parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get GCM temperature and precipitation projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get temperature OLD\n",
    "# forc_dir = config.data_path / 'precip' / 'WRF_frcs_EllsworthCr_forcings'\n",
    "# forc_file = 'forc_46.40625_-123.90625'\n",
    "# forc_cols = ['Year', 'Month', 'Day', 'Hour', 'Precip(mm)', 'Temp(C)', \n",
    "#              'Wind(m/s)', 'SWrad(W/m2)', 'LWrad(W/m2)', 'pressure(kPa)', \n",
    "#              'VaporPress(kPa)']\n",
    "\n",
    "# cols = ['Temp(C)']\n",
    "# inds = [forc_cols.index(x) for x in cols]\n",
    "# arrs = []\n",
    "# sim_dirs = []\n",
    "# for sim_dir in os.listdir(forc_dir):\n",
    "#     if sim_dir == 'pnnl_historical':\n",
    "#         continue\n",
    "#     sim_dirs.append(sim_dir)\n",
    "#     arr = np.loadtxt(forc_dir / sim_dir / forc_file)\n",
    "#     arrs.append(arr[:, inds])\n",
    "# stack = np.column_stack(arrs)\n",
    "# proj_sims_temp = pd.DataFrame(stack, columns=sim_dirs)\n",
    "# date_arr = pd.DataFrame(arr, columns=forc_cols)\n",
    "# proj_sims_temp.index = pd.to_datetime(date_arr[['Year', 'Month', 'Day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get precipitation OLD\n",
    "# wrf_dir = config.data_path / 'precip' / 'VIC_WRF_EllsworthCr'\n",
    "# wrf_file = 'flux_46.40625_-123.90625'\n",
    "# wrf_cols = [\"YEAR\",\"MONTH\",\"DAY\",\"HOUR\",\"OUT_PREC\",\"OUT_PET_SHORT\",\n",
    "#             \"OUT_SWE\",\"OUT_EVAP\",\"OUT_RUNOFF\",\"OUT_BASEFLOW\",\n",
    "#             \"OUT_SOIL_MOIST0\", \"OUT_SOIL_MOIST1\",\"OUT_SOIL_MOIST2\"]\n",
    "\n",
    "# cols = ['OUT_PREC']\n",
    "# inds = [wrf_cols.index(x) for x in cols]\n",
    "# arrs = []\n",
    "# sim_dirs = []\n",
    "# for sim_dir in os.listdir(wrf_dir):\n",
    "#     if sim_dir == 'pnnl_historical':\n",
    "#         continue\n",
    "#     sim_dirs.append(sim_dir)\n",
    "#     arr = np.loadtxt(wrf_dir / sim_dir / 'sim_avg' / '{}.gz'.format(wrf_file))\n",
    "#     arrs.append(arr[:, inds])\n",
    "# stack = np.column_stack(arrs)\n",
    "# proj_sims_ppt = pd.DataFrame(stack, columns=sim_dirs)\n",
    "# date_arr = pd.DataFrame(arr, columns=wrf_cols)\n",
    "# proj_sims_ppt.index = pd.to_datetime(date_arr[['YEAR', 'MONTH', 'DAY']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get precipitation\n",
    "wrf_dir = config.data_path / 'precip' / 'VIC_WRF_EllsworthCr'\n",
    "gcm_avg_dir = wrf_dir / 'sim_avg'\n",
    "cols = ['year', 'month', 'day', 'access1.0_RCP45', 'access1.0_RCP85', 'access1.3_RCP85',\n",
    "        'bcc-csm1.1_RCP85', 'canesm2_RCP85', 'ccsm4_RCP85', 'csiro-mk3.6.0_RCP85',\n",
    "        'fgoals-g2_RCP85', 'gfdl-cm3_RCP85', 'giss-e2-h_RCP85', 'miroc5_RCP85',\n",
    "        'mri-cgcm3_RCP85', 'noresm1-m_RCP85']\n",
    "\n",
    "arr = np.loadtxt(gcm_avg_dir / 'sim_avg_ppt.gz')\n",
    "proj_sims_ppt = pd.DataFrame(data=arr, columns=cols)\n",
    "proj_sims_ppt.index = pd.to_datetime(proj_sims_ppt[['year', 'month', 'day']])\n",
    "proj_sims_ppt = proj_sims_ppt.drop(['year', 'month', 'day'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get temperature\n",
    "forc_dir = config.data_path / 'precip' / 'WRF_frcs_EllsworthCr_forcings'\n",
    "gcm_avg_forc_dir = forc_dir / 'sim_avg'\n",
    "cols = ['year', 'month', 'day', 'access1.0_RCP45', 'access1.0_RCP85', 'access1.3_RCP85',\n",
    "        'bcc-csm1.1_RCP85', 'canesm2_RCP85', 'ccsm4_RCP85', 'csiro-mk3.6.0_RCP85',\n",
    "        'fgoals-g2_RCP85', 'gfdl-cm3_RCP85', 'giss-e2-h_RCP85', 'miroc5_RCP85',\n",
    "        'mri-cgcm3_RCP85', 'noresm1-m_RCP85']\n",
    "\n",
    "arr = np.loadtxt(gcm_avg_forc_dir / 'sim_avg_temp.gz')\n",
    "proj_sims_temp = pd.DataFrame(data=arr, columns=cols)\n",
    "proj_sims_temp.index = pd.to_datetime(proj_sims_temp[['year', 'month', 'day']])\n",
    "proj_sims_temp = proj_sims_temp.drop(['year', 'month', 'day'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804005b2df074b8b8835c530085338d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "colors = sns.color_palette('Dark2', 13)\n",
    "sim_start = pd.to_datetime('01-01-2025')\n",
    "sim_end = pd.to_datetime('12-31-2099')\n",
    "fig = plt.figure(figsize=(9.5, 5))\n",
    "for i, sim in enumerate(proj_sims_temp):\n",
    "    sim_data = proj_sims_temp.loc[:, sim]\n",
    "    sim_subset = sim_data[(sim_data.index >= sim_start) & (sim_data.index <= sim_end)]\n",
    "    sim_group = sim_subset.groupby(pd.Grouper(freq='10Y')).mean()\n",
    "    sim_group.plot(label=sim, color=colors[i])\n",
    "    \n",
    "ensemble_data = proj_sims_temp.mean(axis=1)\n",
    "ensemble_subset = ensemble_data[(ensemble_data.index >= sim_start) & (ensemble_data.index <= sim_end)]\n",
    "ensemble_group = ensemble_subset.groupby(pd.Grouper(freq='10Y')).mean()\n",
    "ensemble_group.plot(label='Average', color='black', linewidth=2.5)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0, 1.2), fancybox=True, ncol=5)\n",
    "plt.ylabel('Temperature (C)')\n",
    "fig.suptitle('Fig. 1: GCM air temperature projections (C)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a1ca8bb5314e5e9396a143e7fbe33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "colors = sns.color_palette('Dark2', 13)\n",
    "sim_start = pd.to_datetime('01-01-2025')\n",
    "sim_end = pd.to_datetime('12-31-2099')\n",
    "fig = plt.figure(figsize=(9.5, 5))\n",
    "for i, sim in enumerate(proj_sims_temp):\n",
    "    sim_data = proj_sims_temp.loc[:, sim]\n",
    "    sim_subset = sim_data[(sim_data.index >= sim_start) & (sim_data.index <= sim_end)]\n",
    "    sim_group = sim_subset.groupby(pd.Grouper(freq='10Y')).mean()\n",
    "    ((sim_group * 9/5)+32).plot(label=sim, color=colors[i])\n",
    "    \n",
    "ensemble_data = proj_sims_temp.mean(axis=1)\n",
    "ensemble_subset = ensemble_data[(ensemble_data.index >= sim_start) & (ensemble_data.index <= sim_end)]\n",
    "ensemble_group = ensemble_subset.groupby(pd.Grouper(freq='10Y')).mean()\n",
    "((ensemble_group*9/5)+32).plot(label='Average', color='black', linewidth=2.5)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0, 1.2), fancybox=True, ncol=5)\n",
    "plt.ylabel('Temperature (F)')\n",
    "fig.suptitle('Fig. 1: GCM air temperature projections (F)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9d19f991824011bf59ab3de4714f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "colors = sns.color_palette('Dark2', 13)\n",
    "sim_start = pd.to_datetime('01-01-2021')\n",
    "sim_end = pd.to_datetime('12-31-2099')\n",
    "fig = plt.figure(figsize=(9.5, 5))\n",
    "for i, sim in enumerate(proj_sims_ppt):\n",
    "    sim_data = proj_sims_ppt.loc[:, sim]\n",
    "    sim_subset = sim_data[(sim_data.index >= sim_start) & (sim_data.index <= sim_end)]\n",
    "    sim_group = sim_subset.groupby(pd.Grouper(freq='Y')).sum()\n",
    "    sim_group.plot(label=sim, color=colors[i])\n",
    "    \n",
    "ensemble_data = proj_sims_ppt.mean(axis=1)\n",
    "ensemble_subset = ensemble_data[(ensemble_data.index >= sim_start) & (ensemble_data.index <= sim_end)]\n",
    "ensemble_group = ensemble_subset.groupby(pd.Grouper(freq='Y')).sum()\n",
    "ensemble_group.plot(label='Average', color='black', linewidth=2.5)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0, 1.2), fancybox=True, ncol=5)\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "fig.suptitle('Fig. 1: GCM precipitation projections')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2658f4b2c5b4a969491d8c1cfec43df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "colors = sns.color_palette('Dark2', 13)\n",
    "sim_start = pd.to_datetime('01-01-2021')\n",
    "sim_end = pd.to_datetime('12-31-2099')\n",
    "fig = plt.figure(figsize=(9.5, 5))\n",
    "for i, sim in enumerate(proj_sims_ppt):\n",
    "    sim_data = proj_sims_ppt.loc[:, sim]\n",
    "    sim_subset = sim_data[(sim_data.index >= sim_start) & (sim_data.index <= sim_end)]\n",
    "    sim_group = sim_subset.groupby(pd.Grouper(freq='m')).sum()\n",
    "    sim_group.plot(label=sim, color=colors[i])\n",
    "    \n",
    "ensemble_data = proj_sims_ppt.mean(axis=1)\n",
    "ensemble_subset = ensemble_data[(ensemble_data.index >= sim_start) & (ensemble_data.index <= sim_end)]\n",
    "ensemble_group = ensemble_subset.groupby(pd.Grouper(freq='m')).sum()\n",
    "ensemble_group.plot(label='Average', color='black', linewidth=2.5)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0, 1.2), fancybox=True, ncol=5)\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "fig.suptitle('Fig. 1: GCM precipitation projections')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1822523200fd49a28d17d23162cb6003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9.5, 5))\n",
    "ensemble_group = ensemble_subset.groupby(pd.Grouper(freq='m')).sum()\n",
    "ensemble_group.plot(label='Average', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling historical temperature normals to GCM projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling temperature with simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_start = pd.to_datetime('01-01-2021')\n",
    "proj_end = pd.to_datetime('12-31-2099')\n",
    "\n",
    "# Get PRISM daily normals\n",
    "piv = pd.pivot_table(temp, columns=temp.index.year, index=temp.index.dayofyear, values='temp')\n",
    "normals_temp_d = piv.mean(axis=1)\n",
    "\n",
    "# Repeat historical daily normals for every year in projected range - these values will be scaled\n",
    "# Merging on day of year will take leap year into account by not merging Feb 29 on non-leap years\n",
    "proj_rng = pd.DataFrame(index=pd.date_range(proj_start, proj_end))\n",
    "proj_rng['doy'] = proj_rng.index.dayofyear\n",
    "proj_temp = proj_rng.merge(pd.DataFrame(normals_temp_d, columns=['normals_temp_d']), \n",
    "               left_on='doy', right_index=True, how='left').drop('doy', axis=1)\n",
    "\n",
    "# Get average GCM daily temps\n",
    "proj_sims_temp_edit = proj_sims_temp[(proj_sims_temp.index >= proj_start) & (proj_sims_temp.index <= proj_end)]\n",
    "gcm_temp_d_mean = pd.DataFrame(proj_sims_temp_edit.groupby(pd.Grouper(freq='d')).mean().mean(axis=1), columns=['gcm_temp_d_mean'])\n",
    "\n",
    "# Convert day of year to signal\n",
    "day = 24 * 60 * 60\n",
    "year = 365.2425 * day\n",
    "timestamp_secs = pd.to_datetime(gcm_temp_d_mean.index)\n",
    "timestamp_secs = timestamp_secs.map(datetime.datetime.timestamp)\n",
    "gcm_temp_d_mean['doy_cos'] = np.cos(timestamp_secs * (2 * np.pi / year))\n",
    "gcm_temp_d_mean['doy_sin'] = np.sin(timestamp_secs * (2 * np.pi / year))\n",
    "gcm_temp_d_mean['year'] = gcm_temp_d_mean.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        gcm_temp_d_mean   R-squared:                       0.946\n",
      "Model:                            OLS   Adj. R-squared:                  0.946\n",
      "Method:                 Least Squares   F-statistic:                 8.528e+04\n",
      "Date:                Fri, 26 Feb 2021   Prob (F-statistic):               0.00\n",
      "Time:                        10:30:54   Log-Likelihood:                -27391.\n",
      "No. Observations:               19332   AIC:                         5.479e+04\n",
      "Df Residuals:                   19327   BIC:                         5.483e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const               -76.6957      0.652   -117.586      0.000     -77.974     -75.417\n",
      "doy_cos              -3.0756      0.044    -69.889      0.000      -3.162      -2.989\n",
      "doy_sin               0.2763      0.021     13.248      0.000       0.235       0.317\n",
      "year                  0.0406      0.000    129.226      0.000       0.040       0.041\n",
      "prism_temp_d_mean     0.4759      0.008     61.110      0.000       0.461       0.491\n",
      "==============================================================================\n",
      "Omnibus:                       80.801   Durbin-Watson:                   1.983\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               97.587\n",
      "Skew:                          -0.089   Prob(JB):                     6.44e-22\n",
      "Kurtosis:                       3.299   Cond. No.                     1.87e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.87e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Train regression model\n",
    "X = gcm_temp_d_mean.copy()\n",
    "y = X.pop('gcm_temp_d_mean')\n",
    "X = sm.add_constant(X)\n",
    "X['normals_temp_d'] = proj_temp['normals_temp_d']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train = sm.add_constant(X_train)\n",
    "olsmodel = sm.OLS(y_train, X_train).fit()\n",
    "print(olsmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = olsmodel.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abef982a4cb84cf6b6848b056a327f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b7184db948>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "((y.groupby(pd.Grouper(freq='m')).mean()*9/5)+32).plot(label='gcm')\n",
    "((X['normals_temp_d'].groupby(pd.Grouper(freq='m')).mean() * 9/5)+32).plot(label='normals')\n",
    "((y_pred.groupby(pd.Grouper(freq='m')).mean()*9/5)+32).plot(label='proj')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method basically reproduces the projection and doesn't use the historical normals as predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling using a constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the [CIG Puget Sound projections report](https://cig.uw.edu/wp-content/uploads/sites/2/2014/11/ps-sok_sec02_climate_2015.pdf), going to scale seasons by a different constant. The constants are projected increases relative to 1970-1999 normals though, so we will need to use that data as the baseline values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdb22eec8d74987a76aa6f32dcc2eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining GCM normals 1990-2020 compared to PRISM normals 1990-2020 to see if they are comparable, before using GCM 1970-1999\n",
    "proj_sims_temp_hist = proj_sims_temp[(proj_sims_temp.index >= temp.index.min()) & (proj_sims_temp.index <= temp.index.max())]\n",
    "gcm_hist_d_mean = pd.DataFrame(proj_sims_temp_hist.groupby(pd.Grouper(freq='d')).mean().mean(axis=1), columns=['gcm_hist_d_mean'])\n",
    "\n",
    "piv = pd.pivot_table(gcm_hist_d_mean, columns=gcm_hist_d_mean.index.year, index=gcm_hist_d_mean.index.dayofyear, values='gcm_hist_d_mean')\n",
    "gcm_normals = piv.mean(axis=1)\n",
    "\n",
    "proj_sims_temp_hist = proj_sims_temp[(proj_sims_temp.index >= pd.to_datetime('01-01-1970')) & \n",
    "                                     (proj_sims_temp.index <= pd.to_datetime('12-31-1999'))]\n",
    "gcm_hist_d_mean = pd.DataFrame(proj_sims_temp_hist.groupby(pd.Grouper(freq='d')).mean().mean(axis=1), columns=['gcm_hist_d_mean'])\n",
    "piv = pd.pivot_table(gcm_hist_d_mean, columns=gcm_hist_d_mean.index.year, index=gcm_hist_d_mean.index.dayofyear, values='gcm_hist_d_mean')\n",
    "gcm_normals_1970 = piv.mean(axis=1)\n",
    "\n",
    "piv = pd.pivot_table(temp, columns=temp.index.year, index=temp.index.dayofyear, values='temp')\n",
    "normals_temp_d = piv.mean(axis=1)\n",
    "\n",
    "normals = pd.concat([normals_temp_d, gcm_normals, gcm_normals_1970], axis=1)\n",
    "normals.columns=['prism_normals', 'gcm_normals', 'gcm_normals_1970']\n",
    "normals.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_start = pd.to_datetime('01-01-2021')\n",
    "proj_end = pd.to_datetime('12-31-2099')\n",
    "\n",
    "# Get GCM daily normals 1970-1999\n",
    "proj_sims_temp_hist = proj_sims_temp[(proj_sims_temp.index >= pd.to_datetime('01-01-1970')) & \n",
    "                                     (proj_sims_temp.index <= pd.to_datetime('12-31-1999'))]\n",
    "gcm_hist_d_mean = pd.DataFrame(proj_sims_temp_hist.groupby(pd.Grouper(freq='d')).mean().mean(axis=1), columns=['gcm_hist_d_mean'])\n",
    "piv = pd.pivot_table(gcm_hist_d_mean, columns=gcm_hist_d_mean.index.year, index=gcm_hist_d_mean.index.dayofyear, values='gcm_hist_d_mean')\n",
    "gcm_normals_d_1970 = piv.mean(axis=1)\n",
    "\n",
    "# Repeat historical daily normals for every year in projected range - these values will be scaled\n",
    "# Merging on day of year will take leap year into account by not merging Feb 29 on non-leap years\n",
    "proj_rng = pd.DataFrame(index=pd.date_range(proj_start, proj_end))\n",
    "proj_rng['doy'] = proj_rng.index.dayofyear\n",
    "proj_temp = proj_rng.merge(pd.DataFrame(gcm_normals_d_1970, columns=['gcm_normals_d_1970']), \n",
    "               left_on='doy', right_index=True, how='left').drop('doy', axis=1)\n",
    "\n",
    "# Get average GCM daily temps\n",
    "proj_sims_temp_edit = proj_sims_temp[(proj_sims_temp.index >= proj_start) & (proj_sims_temp.index <= proj_end)]\n",
    "gcm_temp_d_mean = pd.DataFrame(proj_sims_temp_edit.groupby(pd.Grouper(freq='d')).mean().mean(axis=1), columns=['gcm_temp_d_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate constants\n",
    "proj_years = len(np.unique(pd.date_range(proj_start, proj_end).year))\n",
    "fall_yearly_constant = 7.1 / proj_years\n",
    "winter_yearly_constant = 6.65 / proj_years\n",
    "spring_yearly_constant = 6.6 / proj_years\n",
    "summer_yearly_constant = 8.75 / proj_years\n",
    "\n",
    "# proj_years = len(np.unique(pd.date_range(proj_start, proj_end).year))\n",
    "# fall_yearly_constant = 5 / proj_years\n",
    "# winter_yearly_constant = 5 / proj_years\n",
    "# spring_yearly_constant = 5 / proj_years\n",
    "# summer_yearly_constant = 5 / proj_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply constants\n",
    "constants = []\n",
    "for date in proj_temp.index:\n",
    "    if date.month in [12, 1, 2]:\n",
    "        constants.append(winter_yearly_constant * (date.year - 2020))\n",
    "    if date.month in [3, 4, 5]:\n",
    "        constants.append(spring_yearly_constant * (date.year - 2020))\n",
    "    if date.month in [6, 7, 8]:\n",
    "        constants.append(summer_yearly_constant * (date.year - 2020))\n",
    "    if date.month in [9, 10, 11]:\n",
    "        constants.append(fall_yearly_constant * (date.year - 2020))\n",
    "\n",
    "proj_temp['constants'] = constants\n",
    "proj_temp['proj_temp'] = proj_temp['gcm_normals_d_1970'] + proj_temp['constants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cade8a6c3c4b388297695e8e218831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b7502622c8>"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "gcm_temp_d_mean['gcm_temp_d_mean'].groupby(pd.Grouper(freq='m')).mean().plot(label='gcm average')\n",
    "proj_temp['proj_temp'].groupby(pd.Grouper(freq='m')).mean().plot(label='proj w/ seasonal constants')\n",
    "proj_temp['gcm_normals_d_1970'].groupby(pd.Grouper(freq='m')).mean().plot(label='normals')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7f7f89781f4e9683b34883e9ae72fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b750d0f6c8>"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "((gcm_temp_d_mean['gcm_temp_d_mean'].groupby(pd.Grouper(freq='m')).mean()* 9/5)+32).plot(label='gcm average')\n",
    "((proj_temp['proj_temp'].groupby(pd.Grouper(freq='m')).mean()* 9/5)+32).plot(label='proj w/ seasonal constants')\n",
    "((proj_temp['gcm_normals_d_1970'].groupby(pd.Grouper(freq='m')).mean()* 9/5)+32).plot(label='normals')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b219961beb1e4b4db00c12a3ef05c9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b750d1d0c8>"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "((gcm_temp_d_mean['gcm_temp_d_mean'].groupby(pd.Grouper(freq='y')).mean()* 9/5)+32).plot(label='gcm average')\n",
    "((proj_temp['proj_temp'].groupby(pd.Grouper(freq='y')).mean()* 9/5)+32).plot(label='proj w/ seasonal constants')\n",
    "((proj_temp['gcm_normals_d_1970'].groupby(pd.Grouper(freq='y')).mean()* 9/5)+32).plot(label='normals')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does scaling with seasonal constants lead to such a higher projected temperature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling historical precipitation normals to GCM projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling using monthly means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_start = pd.to_datetime('01-01-2021')\n",
    "proj_end = pd.to_datetime('12-31-2099')\n",
    "\n",
    "# Get PRISM daily normals\n",
    "piv_d = pd.pivot_table(prism_precip_mean, columns=prism_precip_mean.index.year, index=prism_precip_mean.index.dayofyear, values='avg_precip')\n",
    "normals_precip_d = piv_d.mean(axis=1)\n",
    "\n",
    "# Repeat daily normals for every year in projected range - these values will be scaled\n",
    "# Merging on day of year will take leap year into account by not merging Feb 29 on non-leap years\n",
    "proj_rng = pd.DataFrame(index=pd.date_range(proj_start, proj_end))\n",
    "proj_rng['doy'] = proj_rng.index.dayofyear\n",
    "proj_precip = proj_rng.merge(pd.DataFrame(normals_precip_d, columns=['normals_precip_d']), \n",
    "               left_on='doy', right_index=True, how='left').drop('doy', axis=1)\n",
    "\n",
    "# Get PRISM monthly normals\n",
    "proj_years = len(pd.date_range(proj_start, proj_end, freq='y'))\n",
    "piv_m = pd.pivot_table(prism_precip_mean, columns=prism_precip_mean.index.year, index=prism_precip_mean.index.month, values='avg_precip', aggfunc=np.sum)\n",
    "# normals_m = pd.DataFrame(piv_m.mean(axis=1), columns=['normals_m'])\n",
    "normals_m = np.tile(piv_m.mean(axis=1), proj_years)\n",
    "\n",
    "# # Join daily and monthly normals\n",
    "# proj_precip.index = proj_precip.index.strftime('%m')\n",
    "# proj_precip.join(normals_m, how='left')\n",
    "\n",
    "# Get GCM monthly means\n",
    "gcm_precip_m = proj_sims_ppt.groupby(pd.Grouper(freq='M')).sum()\n",
    "gcm_precip_m = gcm_precip_m[(gcm_precip_m.index >= proj_start) & (gcm_precip_m.index <= proj_end)]\n",
    "gcm_precip_m_mean = gcm_precip_m.mean(axis=1)\n",
    "\n",
    "# For each projected month, find the % increase from historical PRISM to projected GCM\n",
    "scale_m = (gcm_precip_m_mean- normals_m) / normals_m\n",
    "scale_m_precip = pd.DataFrame(scale_m, columns=['scale_m_precip'])\n",
    "\n",
    "# Apply scaling factor to historical daily normals to get projected values\n",
    "proj_precip.index = proj_precip.index.strftime('%Y-%m')\n",
    "scale_m_precip.index = scale_m_precip.index.strftime('%Y-%m')\n",
    "proj_precip_scaled = proj_precip.merge(scale_m_precip, left_index=True, right_index=True, how='left')\n",
    "proj_precip_scaled['proj_precip_scaled_m'] = proj_precip_scaled['normals_precip_d'] + (proj_precip_scaled['normals_precip_d'] * proj_precip_scaled['scale_m_precip'])\n",
    "proj_precip_scaled.index = proj_rng.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978bd88e2db94f2e8d5af5724a59787c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Projected precipitation scaled using monthly means')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normals_m_df = pd.DataFrame(normals_m, index=gcm_precip_m_mean.index)\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = proj_precip_scaled.groupby(pd.Grouper(freq='Y')).sum()['proj_precip_scaled_m'].plot()\n",
    "normals_m_df.groupby(pd.Grouper(freq='Y')).sum().plot(ax=ax)\n",
    "fig.suptitle('Projected precipitation scaled using monthly means')\n",
    "# Perhaps using monthly means instead of yearly means allows for too much variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling using yearly means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_start = pd.to_datetime('01-01-2020')\n",
    "proj_end = pd.to_datetime('12-31-2099')\n",
    "\n",
    "# Get GCM yearly mean\n",
    "gcm_ppt_y_mean = proj_sims_ppt.groupby(pd.Grouper(freq='Y')).sum().mean(axis=1)\n",
    "gcm_ppt_y_mean = gcm_ppt_y_mean[(gcm_ppt_y_mean.index >= proj_start) & (gcm_ppt_y_mean.index <= proj_end)]\n",
    "\n",
    "# Get PRISM daily mean\n",
    "piv = pd.pivot_table(prism_precip_mean, columns=prism_precip_mean.index.year, index=prism_precip_mean.index.dayofyear, values='avg_precip')\n",
    "prism_precip_d_mean = piv.mean(axis=1)\n",
    "\n",
    "# Get PRISM yearly mean\n",
    "prism_precip_y_mean = prism_precip_mean.groupby(pd.Grouper(freq='Y')).sum()['avg_precip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get yearly mean of historical daily normals\n",
    "hist_precip_y_mean = proj_precip_scaled['normals_precip_d'].groupby(pd.Grouper(freq='Y')).mean()\n",
    "\n",
    "# For each projected month, find the % increase from historical PRISM to projected GCM\n",
    "scale_y = (gcm_ppt_y_mean- hist_precip_y_mean) / hist_precip_y_mean\n",
    "scale_y_precip = pd.DataFrame(scale_y, columns=['scale_y_precip'])\n",
    "\n",
    "# Apply scaling factor to historical daily normals to get projected values\n",
    "proj_precip_scaled.index = proj_precip_scaled.index.strftime('%Y')\n",
    "scale_y_precip.index = scale_y_precip.index.strftime('%Y')\n",
    "proj_precip_scaled = proj_precip_scaled.merge(scale_y_precip, left_index=True, right_index=True, how='left')\n",
    "proj_precip_scaled\n",
    "proj_precip_scaled['proj_precip_scaled_y'] = proj_precip_scaled['normals_precip_d'] + (proj_precip_scaled['normals_precip_d'] * proj_precip_scaled['scale_y_precip'])\n",
    "proj_precip_scaled.index = proj_rng.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f08686528ed4deabd6f67fba3e697fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b7541788c8>"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = proj_precip_scaled.groupby(pd.Grouper(freq='Y')).sum()\n",
    "x['year'] = x.index.year\n",
    "m, b = np.polyfit(x['year'], x['proj_precip_scaled_m'], 1)\n",
    "z = m*x['year']+b\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "proj_precip_scaled['proj_precip_scaled_y'].groupby(pd.Grouper(freq='Y')).mean().plot(label='Proj yearly mean (scaled with yearly)')\n",
    "proj_precip_scaled.groupby(pd.Grouper(freq='Y')).sum()['proj_precip_scaled_m'].plot(label='Proj yearly mean (scaled with monthly)')\n",
    "z.plot(label='Projected trend')\n",
    "proj_precip_scaled['normals_precip_d'].groupby(pd.Grouper(freq='Y')).sum().plot(label='Normals yearly mean')\n",
    "fig.suptitle('Projected precipitation scaled with monthly and yearly means')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are still very naive scaled projections. Want some way to capture (1) seasonal variation of temperature, which is projected to rise higher in summer than in winter, and (2) seasonal variation of precipitation which is projected to lower for summer and rise for winter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling using bias-correction for temperature\n",
    "\n",
    "http://www.ccafs-climate.org/bias_correction/#Bias_correction\n",
    "\n",
    "$$ T_{BC}(t) = \\overline{O_{ref}} + \\frac{\\sigma_{O,ref}}{\\sigma_{T,ref}}(T_{raw}(t) - \\overline{T_{ref}}) $$ \n",
    "\n",
    "Might want to do the individual GCMs\n",
    "\n",
    "Need to have reference and future periods of equal length. So will do chunks to match 30 year normals: 2010-2039, 2040-2069, 2070-2099\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = [pd.to_datetime(x) for x in ['01-01-2010', '01-01-2040', '01-01-2070']]\n",
    "ends = [pd.to_datetime(x) for x in ['12-31-2039', '12-31-2069', '12-31-2099']]\n",
    "\n",
    "normals_start = pd.to_datetime('1981-01-01')\n",
    "normals_end = pd.to_datetime('2010-12-31')\n",
    "\n",
    "# Get PRISM daily normals\n",
    "z = temp[(temp.index >= normals_start) & (temp.index <= normals_end)]\n",
    "piv_d = pd.pivot_table(z, columns=z.index.year, \n",
    "                       index=z.index.dayofyear, values='temp')\n",
    "normals_temp_d = piv_d.mean(axis=1)\n",
    "\n",
    "# Get historical daily normals from GCM\n",
    "proj_sims_temp_hist = proj_sims_temp[(proj_sims_temp.index >= z.index.min()) & \n",
    "                                     (proj_sims_temp.index <= z.index.max())]\n",
    "gcm_hist_d_mean = pd.DataFrame(proj_sims_temp_hist.groupby(pd.Grouper(freq='d')).mean().mean(axis=1), columns=['gcm_hist_d_mean'])\n",
    "piv = pd.pivot_table(gcm_hist_d_mean, columns=gcm_hist_d_mean.index.year, index=gcm_hist_d_mean.index.dayofyear, values='gcm_hist_d_mean')\n",
    "gcm_normals_d = piv.mean(axis=1)\n",
    "\n",
    "proj_sims_temp_d = proj_sims_temp.groupby(pd.Grouper(freq='d')).mean()\n",
    "o_ref_mean = np.mean(normals_temp_d)\n",
    "sigma_o_ref = np.std(normals_temp_d)\n",
    "t_ref_mean = np.mean(gcm_normals_d)\n",
    "sigma_t_ref = np.std(gcm_normals_d)\n",
    "\n",
    "t_raw_list = []\n",
    "t_bc_list = [] \n",
    "for start, end in zip(starts, ends):\n",
    "    t_raw_sims = proj_sims_temp_d[(proj_sims_temp_d.index >= start) & \n",
    "                            (proj_sims_temp_d.index <= end)]\n",
    "    t_raw_chunk = t_raw_sims.mean(axis=1)\n",
    "    \n",
    "    t_bc_chunk = o_ref_mean + (sigma_o_ref / sigma_t_ref)*(t_raw_chunk - t_ref_mean)\n",
    "    \n",
    "    t_raw_list.append(t_raw_chunk)\n",
    "    t_bc_list.append(t_bc_chunk)\n",
    "\n",
    "rng = pd.date_range(starts[0], ends[2])\n",
    "t_bc = pd.DataFrame(data=np.concatenate(t_bc_list, axis=0), columns=['t_bc'], index=rng)\n",
    "t_raw = pd.DataFrame(data=np.concatenate(t_raw_list, axis=0), columns=['t_raw'], index=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Means of raw projected chunks: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[10.360872703435106, 11.400637302216856, 12.708300550754348]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Means of historical temp chunks: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[10.646711049501109, 11.741296179601669, 13.117904643387789]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('Means of raw projected chunks: ', [x.mean() for x in t_raw_list])\n",
    "display('Means of historical temp chunks: ', [x.mean() for x in t_bc_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f049b2a47d4613814a90dfba50703f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(6, 4))\n",
    "axes[0].title.set_text('Yearly average')\n",
    "axes[0].plot(t_bc.groupby(pd.Grouper(freq='y')).mean(), label=\"Bias-correction scaled\")\n",
    "axes[0].plot(t_raw.groupby(pd.Grouper(freq='y')).mean(), label=\"Ensemble average\")\n",
    "axes[1].title.set_text('Daily average')\n",
    "axes[1].plot(t_bc)\n",
    "axes[1].plot(t_raw)\n",
    "axes[0].legend()\n",
    "axes[0].get_xaxis().set_visible(False)\n",
    "plt.ylabel('Degrees (C)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling using change factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = [pd.to_datetime(x) for x in ['01-01-2010', '01-01-2040', '01-01-2070']]\n",
    "ends = [pd.to_datetime(x) for x in ['12-31-2039', '12-31-2069', '12-31-2099']]\n",
    "\n",
    "normals_start = pd.to_datetime('1981-01-01')\n",
    "normals_end = pd.to_datetime('2010-12-31')\n",
    "\n",
    "# Get PRISM daily normals\n",
    "normals_start = pd.to_datetime('1981-01-01')\n",
    "normals_end = pd.to_datetime('2010-12-31')\n",
    "\n",
    "# Get PRISM daily normals\n",
    "z = temp[(temp.index >= normals_start) & (temp.index <= normals_end)]\n",
    "piv_d = pd.pivot_table(z, columns=z.index.year, \n",
    "                       index=z.index.dayofyear, values='temp')\n",
    "normals_temp_d = piv_d.mean(axis=1)\n",
    "\n",
    "# Get historical daily normals from GCM\n",
    "proj_sims_temp_hist = proj_sims_temp[(proj_sims_temp.index >= prism_precip_mean.index.min()) & \n",
    "                                     (proj_sims_temp.index <= prism_precip_mean.index.max())]\n",
    "gcm_hist_d_mean = pd.DataFrame(proj_sims_temp_hist.groupby(pd.Grouper(freq='d')).mean().mean(axis=1), columns=['gcm_hist_d_mean'])\n",
    "piv = pd.pivot_table(gcm_hist_d_mean, columns=gcm_hist_d_mean.index.year, index=gcm_hist_d_mean.index.dayofyear, values='gcm_hist_d_mean')\n",
    "gcm_normals_d = piv.mean(axis=1)\n",
    "\n",
    "proj_sims_temp_d = proj_sims_temp.groupby(pd.Grouper(freq='d')).mean()\n",
    "o_ref = z['temp']\n",
    "sigma_o_ref = np.std(normals_temp_d)\n",
    "t_ref_mean = np.mean(gcm_normals_d)\n",
    "sigma_t_ref = np.std(gcm_normals_d)\n",
    "\n",
    "t_raw_list = []\n",
    "t_cf_list = [] \n",
    "for start, end in zip(starts, ends):\n",
    "    t_raw_sims = proj_sims_temp_d[(proj_sims_temp_d.index >= start) & \n",
    "                            (proj_sims_temp_d.index <= end)]\n",
    "    t_raw_chunk = t_raw_sims.mean(axis=1)\n",
    "    t_raw_mean = np.mean(t_raw_chunk)\n",
    "    sigma_t_raw = np.std(t_raw_chunk)\n",
    "    t_cf_chunk = t_raw_mean + (sigma_t_raw / sigma_t_ref)*(o_ref - t_ref_mean)\n",
    "    \n",
    "    t_raw_list.append(t_raw_chunk)\n",
    "    t_cf_list.append(t_cf_chunk.values)\n",
    "\n",
    "# Need to tack on an extra day to account for misaligned leap years between o_ref and t_raw\n",
    "t_cf_list[0] = np.append(t_cf_list[0], t_cf_list[0][-1])\n",
    "rng = pd.date_range(starts[0], ends[2])\n",
    "t_cf = pd.DataFrame(data=np.concatenate(t_cf_list, axis=0), columns=['t_cf'], index=rng)\n",
    "t_raw = pd.DataFrame(data=np.concatenate(t_raw_list, axis=0), columns=['t_raw'], index=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Means of raw projected chunks: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[10.360872703435106, 11.400637302216856, 12.708300550754348]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Means of historical temp chunks: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[10.443930393547777, 11.488464089681786, 12.796621482233402]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('Means of raw projected chunks: ', [x.mean() for x in t_raw_list])\n",
    "display('Means of historical temp chunks: ', [x.mean() for x in t_cf_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8972acc2dc274888815d6fee4319c18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(6, 4))\n",
    "axes[0].title.set_text('Yearly average')\n",
    "axes[0].plot(t_cf.groupby(pd.Grouper(freq='y')).mean(), label=\"Change factor scaled\")\n",
    "axes[0].plot(t_raw.groupby(pd.Grouper(freq='y')).mean(), label=\"Ensemble average\")\n",
    "axes[1].title.set_text('Daily average')\n",
    "axes[1].plot(t_cf)\n",
    "axes[1].plot(t_raw)\n",
    "axes[0].legend()\n",
    "axes[0].get_xaxis().set_visible(False)\n",
    "plt.ylabel('Degrees (C)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change factor method looks superior to bias correction because it contains more variation - natural heat and cold waves that were present in the 1981-2010 data, but scaled up so that the means are roughly equal to the projected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "velma_start = pd.to_datetime('01-01-2005')\n",
    "velma_end = pd.to_datetime('12-31-2099')\n",
    "\n",
    "# Export ensemble average temperature, 2022 - 2099\n",
    "ensemble_temp = proj_sims_temp[(proj_sims_temp.index >= velma_start) & (proj_sims_temp.index <= velma_end)]\n",
    "ensemble_temp_mean = ensemble_temp.mean(axis=1)\n",
    "outfile = config.velma_data / 'temp' / 'gcm_avg_temp_05_99.csv'\n",
    "ensemble_temp_mean.to_csv(outfile, header=False, index=False)\n",
    "\n",
    "# Export ensemble average precipitation, 2022 - 2099\n",
    "ensemble_precip = proj_sims_ppt[(proj_sims_ppt.index >= velma_start) & (proj_sims_ppt.index <= velma_end)]\n",
    "ensemble_precip_mean = ensemble_precip.mean(axis=1)\n",
    "outfile = config.velma_data / 'precip' / 'gcm_avg_precip_05_99.csv'\n",
    "ensemble_precip_mean.to_csv(outfile, header=False, index=False)\n",
    "\n",
    "# Export 1990-2020 normals, repeated to 2099\n",
    "# Get PRISM daily normals\n",
    "piv_d = pd.pivot_table(prism_precip_mean, columns=prism_precip_mean.index.year, index=prism_precip_mean.index.dayofyear, values='avg_precip')\n",
    "normals_precip_d = piv_d.mean(axis=1)\n",
    "# Repeat daily normals for every year in projected range - these values will be scaled\n",
    "# Merging on day of year will take leap year into account by not merging Feb 29 on non-leap years\n",
    "proj_rng = pd.DataFrame(index=pd.date_range(proj_start, proj_end))\n",
    "proj_rng['doy'] = proj_rng.index.dayofyear\n",
    "proj_precip = proj_rng.merge(pd.DataFrame(normals_precip_d, columns=['normals_precip_d']), \n",
    "               left_on='doy', right_index=True, how='left').drop('doy', axis=1)\n",
    "outfile = config.velma_data / 'precip' / 'prism_gauge_avg_1990_20_normals_05_99.csv'\n",
    "proj_precip.to_csv(outfile, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating 2000-2019 precipitation to 2099\n",
    "hist_start = pd.to_datetime('01-01-2000')\n",
    "hist_end = pd.to_datetime('12-31-2019')\n",
    "proj_years = len(np.unique(pd.date_range(proj_start, proj_end).year))\n",
    "\n",
    "z = prism_precip_mean[(prism_precip_mean.index >= hist_start) & (prism_precip_mean.index <= hist_end)]['avg_precip']\n",
    "x = np.tile(z, int(proj_years / len(np.unique(z.index.year))))\n",
    "precip_tiled = pd.DataFrame(x, columns=['avg_precip'], index=pd.date_range(proj_start, proj_end))\n",
    "\n",
    "outfile = config.velma_data / 'precip' / 'prism_gauge_avg_20_99.csv'\n",
    "precip_tiled.to_csv(outfile, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export change factor scaled temperature\n",
    "t_cf_proj = t_cf[(t_cf.index >= proj_start) & (t_cf.index <= proj_end)]\n",
    "outfile = config.velma_data / 'temp' / 'prism_temp_cf_20_99.csv'\n",
    "t_cf.to_csv(outfile, header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnc_velma",
   "language": "python",
   "name": "tnc_velma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
