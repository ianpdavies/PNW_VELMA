{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greatest-roads",
   "metadata": {},
   "source": [
    "# Historical Ellsworth (1984-2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "featured-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import __init__\n",
    "import scripts.config as config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import datetime\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from statsmodels.iolib.smpickle import load_pickle\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outstanding-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting parameters\n",
    "\n",
    "XSMALL_SIZE = 6\n",
    "SMALL_SIZE = 7\n",
    "MEDIUM_SIZE = 9\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "structured-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcms = ['canesm2_RCP85', 'ccsm4_RCP85', 'giss_e2_h_RCP85', 'noresm1_m_RCP85', 'PRISM']\n",
    "\n",
    "# Import driver data\n",
    "sim_start = pd.to_datetime('01-01-1984')\n",
    "sim_end = pd.to_datetime('12-31-2020')\n",
    "sim_range = pd.date_range(sim_start, sim_end)\n",
    "\n",
    "temp_files = []\n",
    "precip_files = []\n",
    "for gcm in [x for x in gcms if x!='PRISM']:\n",
    "    temp_file_path = config.velma_data / 'temp' / '{}_{}_{}_temp.csv'.format(gcm, \n",
    "                                                                             sim_start.year % 100,\n",
    "                                                                             sim_end.year % 100)\n",
    "    temp_file = pd.read_csv(temp_file_path, names=['temp'])\n",
    "    temp_file.index = sim_range\n",
    "    temp_files.append(temp_file)\n",
    "    \n",
    "    precip_file_path = config.velma_data / 'precip' / '{}_{}_{}_ppt.csv'.format(gcm, \n",
    "                                                                                sim_start.year % 100, \n",
    "                                                                                sim_end.year % 100)\n",
    "    precip_file = pd.read_csv(precip_file_path, names=['precip'])\n",
    "    precip_file.index = sim_range\n",
    "    precip_files.append(precip_files)\n",
    "\n",
    "\n",
    "# PRISM data has a different naming convention because the precip is gauge/PRISM average\n",
    "temp_file_path = config.velma_data / 'temp' / 'PRISM_{}_{}_temp.csv'.format(sim_start.year % 100, \n",
    "                                                                            sim_end.year % 100)\n",
    "temp_file = pd.read_csv(temp_file_path, names=['temp'])\n",
    "temp_file.index = sim_range\n",
    "temp_files.append(temp_file)\n",
    "\n",
    "precip_file_path = config.velma_data / 'precip' / 'PRISM_{}_{}_gauge_avg_ppt.csv'.format(sim_start.year % 100, \n",
    "                                                                                         sim_end.year % 100)\n",
    "precip_file = pd.read_csv(precip_file_path, names=['precip'])\n",
    "precip_file.index = sim_range\n",
    "precip_files.append(precip_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "finished-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import results files\n",
    "\n",
    "# scenarios = ['baseline', 'baseline_water_balance_errors']\n",
    "# scenarios = ['baseline', 'ind35yr', 'activeall', 'historical']\n",
    "scenarios = ['historical']\n",
    "\n",
    "dailies = []\n",
    "annuals = []\n",
    "\n",
    "# Import daily and annual results\n",
    "# Results in nested lists ([x][y], for x management scenarios and y GCMs)\n",
    "for scenario in scenarios:\n",
    "    dailies_scenario = []\n",
    "    annuals_scenario = []\n",
    "    scenario_dir = config.velma_data.parents[1] / 'results' / scenario\n",
    "    dirs = os.listdir(scenario_dir)\n",
    "    for directory in dirs:\n",
    "        results_dir = scenario_dir / 'ellsworth_{}_{}_{}_{}'.format(scenario,\n",
    "                                                                    sim_start.year % 100,\n",
    "                                                                    sim_end.year % 100,\n",
    "                                                                    gcm)\n",
    "        results_dir = scenario_dir / directory\n",
    "\n",
    "        daily_results = pd.read_csv(results_dir / 'DailyResults.csv')\n",
    "\n",
    "        # Format datetime\n",
    "        jday_pad = daily_results['Day'].apply(lambda x: str(x).zfill(3))\n",
    "        str_year = daily_results['Year'].apply(lambda x: str(x))\n",
    "        rng = pd.to_datetime((str_year + jday_pad), format='%Y%j')\n",
    "        daily_results.index = rng\n",
    "        dailies_scenario.append(daily_results)\n",
    "\n",
    "        annual_results = pd.read_csv(results_dir / 'AnnualResults.csv')\n",
    "        annuals_scenario.append(annual_results)\n",
    "        \n",
    "    dailies.append(dailies_scenario)\n",
    "    annuals.append(annuals_scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-payroll",
   "metadata": {},
   "source": [
    "## Runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "empty-replication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5edb6e01484d4cb6b0d62ba8feaddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Daily runoff time series (all years together)\n",
    "\n",
    "# # Check that all dfs have same number of columns\n",
    "# all([len(dailies[0].columns.intersection(df.columns)) \n",
    "#       == dailies[0].shape[1] for df in dailies])\n",
    "\n",
    "# colors = sns.color_palette('tab10', len(sims))\n",
    "\n",
    "fig, axes = plt.subplots(ncols=1, nrows=4, figsize=(5, 6))\n",
    "\n",
    "z = [x.groupby(pd.Grouper(freq='7d')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[0]]\n",
    "yearly_7day_min = pd.concat([x.groupby(pd.Grouper(freq='Y')).min() for x in z], axis=1)\n",
    "yearly_7day_min.columns = gcms\n",
    "yearly_7day_min.plot(ax=axes[0], linewidth=0.8)\n",
    "axes[0].title.set_text('Yearly min 7-day flow')\n",
    "\n",
    "yearly_7day_max = pd.concat([x.groupby(pd.Grouper(freq='Y')).max() for x in z], axis=1)\n",
    "yearly_7day_max.columns = gcms\n",
    "yearly_7day_max.plot(ax=axes[1], linewidth=0.8)\n",
    "axes[1].title.set_text('Yearly max 7-day flow')\n",
    "axes[1].get_legend().remove()\n",
    "\n",
    "z = [x.groupby(pd.Grouper(freq='Y')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[0]]\n",
    "yearly_sum = pd.concat(z, axis=1)\n",
    "yearly_sum.columns = gcms\n",
    "yearly_sum.plot(ax=axes[2], linewidth=0.8)\n",
    "axes[2].title.set_text('Yearly sum flow')\n",
    "axes[2].get_legend().remove()\n",
    "\n",
    "z = [x.groupby(pd.Grouper(freq='Y')).mean()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[0]]\n",
    "yearly_mean = pd.concat(z, axis=1)\n",
    "yearly_mean.columns = gcms\n",
    "yearly_mean.plot(ax=axes[3], linewidth=0.8)\n",
    "axes[3].title.set_text('Yearly mean flow')\n",
    "axes[3].get_legend().remove()\n",
    "\n",
    "plt.ylabel('Runoff (mm)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "banner-picnic",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba613120a8e4593b69a34668adb1cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "# Averaged GCM flows for all management scenarios\n",
    "fig, axes = plt.subplots(ncols=2, nrows=4, figsize=(7, 6))\n",
    "combos = list(itertools.product(gcms, scenarios))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x.groupby(pd.Grouper(freq='7d')).mean()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[i]]\n",
    "    yearly_7day_min = pd.concat([x.groupby(pd.Grouper(freq='y')).min() for x in z], axis=1)\n",
    "    yearly_7day_min.columns = gcms\n",
    "    yearly_7day_min.plot(ax=axes[0, 0], linewidth=0.7)\n",
    "    yearly_7day_min.mean(axis=1).plot(ax=axes[0, 1], linewidth=0.9, label=scenario)\n",
    "    axes[0, 0].title.set_text('Yearly min 7-day flow')\n",
    "    axes[0, 1].title.set_text('Yearly min, GCM means')\n",
    "    axes[0, 0].legend(loc='upper left', bbox_to_anchor=(0, 1.7), fancybox=True, ncol=3, fontsize='small')\n",
    "    axes[0, 0].set_ylim(0, 2)\n",
    "    axes[0, 1].set_ylim(0, 2)\n",
    "\n",
    "    yearly_7day_max = pd.concat([x.groupby(pd.Grouper(freq='y')).max() for x in z], axis=1)\n",
    "    yearly_7day_max.columns = gcms\n",
    "    yearly_7day_max.plot(ax=axes[1, 0], linewidth=0.7)\n",
    "    yearly_7day_max.mean(axis=1).plot(ax=axes[1, 1], linewidth=0.9, label=scenario)\n",
    "    axes[1, 0].title.set_text('Yearly max 7-day flow')\n",
    "    axes[1, 1].title.set_text('Yearly max, GCM means')\n",
    "    axes[1, 0].get_legend().remove()\n",
    "\n",
    "    z = [x.groupby(pd.Grouper(freq='y')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[i]]\n",
    "    yearly_sum = pd.concat(z, axis=1)\n",
    "    yearly_sum.columns = gcms\n",
    "    yearly_sum.plot(ax=axes[2, 0], linewidth=0.7)\n",
    "    yearly_sum.mean(axis=1).plot(ax=axes[2, 1], linewidth=0.9, label=scenario)\n",
    "    axes[2, 0].title.set_text('Yearly sum flow')\n",
    "    axes[2, 1].title.set_text('Yearly sum, GCM means')\n",
    "    axes[2, 0].get_legend().remove()\n",
    "\n",
    "    z = [x.groupby(pd.Grouper(freq='y')).mean()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[i]]\n",
    "    yearly_mean = pd.concat(z, axis=1)\n",
    "    yearly_mean.columns = gcms\n",
    "    yearly_mean.plot(ax=axes[3, 0], linewidth=0.7) \n",
    "    yearly_mean.mean(axis=1).plot(ax=axes[3, 1], linewidth=0.9, label=scenario)\n",
    "    axes[3, 0].title.set_text('Yearly mean flow')\n",
    "    axes[3, 1].title.set_text('Yearly mean, GCM means')\n",
    "    axes[3, 0].get_legend().remove()\n",
    "\n",
    "axes[0, 1].legend(loc='upper right', bbox_to_anchor=(1, 1.5), fancybox=True, ncol=3)\n",
    "axes[0, 0].set_ylabel('Runoff (mm)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-settlement",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Stream temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "infrared-stanford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\tnc_velma\\notebooks\\..\\results\\historical\\ellsworth_historical_84_20_canesm2_RCP85\\Cell_i95156_x284_y236_node_1 - Copy.csv\n",
      "C:\\Users\\ipdavies\\tnc_velma\\notebooks\\..\\results\\historical\\ellsworth_historical_84_20_ccsm4_RCP85\\Cell_i95156_x284_y236_node_1.csv\n",
      "C:\\Users\\ipdavies\\tnc_velma\\notebooks\\..\\results\\historical\\ellsworth_historical_84_20_giss_e2_h_RCP85\\Cell_i95156_x284_y236_node_1.csv\n",
      "C:\\Users\\ipdavies\\tnc_velma\\notebooks\\..\\results\\historical\\ellsworth_historical_84_20_noresm1_m_RCP85\\Cell_i95156_x284_y236_node_1.csv\n",
      "C:\\Users\\ipdavies\\tnc_velma\\notebooks\\..\\results\\historical\\ellsworth_historical_84_20_PRISM\\Cell_i95156_x284_y236_node_1.csv\n"
     ]
    }
   ],
   "source": [
    "# Get cell writer files\n",
    "\n",
    "cell_results = []\n",
    "for scenario in scenarios:\n",
    "    cell_results_scenario = []\n",
    "    scenario_dir = config.velma_data.parents[1] / 'results' / scenario\n",
    "    for gcm in gcms:\n",
    "        cell_paths = []\n",
    "        results_dir = scenario_dir / 'ellsworth_{}_{}_{}_{}'.format(scenario,\n",
    "                                                                    sim_start.year % 100,\n",
    "                                                                    sim_end.year % 100,\n",
    "                                                                    gcm)\n",
    "        for file in os.listdir(results_dir):\n",
    "            if file.startswith('Cell_'):\n",
    "                cell_paths.append(file)\n",
    "        \n",
    "        nodes = []\n",
    "        for path in cell_paths:\n",
    "            nodes.append(path.split('_')[-1])\n",
    "        \n",
    "        cell_paths_sorted = [x for _,x in natsorted(zip(nodes,cell_paths))]\n",
    "        \n",
    "        for path in [cell_paths_sorted[0]]:  # Only need the first cell, which is the Ellsworth mouth/outlet\n",
    "            cell_result = pd.read_csv(results_dir / path)\n",
    "            jday_pad = cell_result['Jday'].apply(lambda x: str(x).zfill(3))\n",
    "            str_year = cell_result['Year'].apply(lambda x: str(x))\n",
    "            cell_result['date'] = str_year + jday_pad\n",
    "            rng = pd.to_datetime(cell_result['date'], format='%Y%j')\n",
    "            cell_result.index = rng\n",
    "            cell_results_scenario.append(cell_result)\n",
    "    \n",
    "    cell_results.append(cell_results_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "stretch-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct VELMA stream temperature seasonal bias using pre-trained regression model\n",
    "# *** Not sure if this correction is still valid considering the non-linear seasonal changes of the climate projections ***\n",
    "\n",
    "olsmodel = load_pickle(config.data_path.parents[0] / 'models' / 'stream_temp_correction_ols.pickle')\n",
    "\n",
    "stream_temps_corrected = []\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    stream_temps_scenario = []\n",
    "    for j, gcm in enumerate(gcms):\n",
    "        z = cell_results[i][j]['Water_Surface_Temperature(degrees_C)']\n",
    "        \n",
    "        day = 24 * 60 * 60\n",
    "        year = 365.2425 * day\n",
    "        timestamp_secs = pd.to_datetime(z.index)\n",
    "        timestamp_secs = timestamp_secs.map(datetime.datetime.timestamp)\n",
    "        year_cos = np.cos(timestamp_secs * (2 * np.pi / year))\n",
    "        year_sin = np.sin(timestamp_secs * (2 * np.pi / year))\n",
    "\n",
    "        y = pd.DataFrame(data=np.column_stack([z, year_cos, year_sin]), columns=['temp', 'year_cos', 'year_sin'])\n",
    "        y.index = z.index\n",
    "        y['air_temp_3day_avg'] = y['temp'].rolling(3, min_periods=0).mean()\n",
    "\n",
    "        y = sm.add_constant(y)\n",
    "        y['streamtemp_corrected'] = olsmodel.predict(y)\n",
    "        \n",
    "        stream_temps_scenario.append(y['streamtemp_corrected'])\n",
    "    \n",
    "    stream_temps_corrected.append(stream_temps_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "physical-french",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d4154a33674d50b52072dd7075f291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "fig, axes = plt.subplots(ncols=2, nrows=3, figsize=(7, 5))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x for x in stream_temps_corrected[i]]\n",
    "    stream_temp = pd.concat([x.groupby(pd.Grouper(freq='y')).mean() for x in z], axis=1)\n",
    "    stream_temp.columns = gcms\n",
    "    stream_temp.plot(ax=axes[0, 0], linewidth=0.7, label=scenario)\n",
    "    axes[0, 0].title.set_text('Yearly mean')\n",
    "    axes[0, 0].legend(loc='upper left', bbox_to_anchor=(0, 1.7), fancybox=True, ncol=3, fontsize='small')\n",
    "    axes[0, 1].plot(stream_temp.mean(axis=1), label=scenario)\n",
    "    axes[0, 1].title.set_text('Yearly mean, GCM means')\n",
    "    \n",
    "    z = [x.groupby(pd.Grouper(freq='7d')).mean() for x in stream_temps_corrected[i]]\n",
    "    stream_temp = pd.concat([x.groupby(pd.Grouper(freq='y')).min() for x in z], axis=1)\n",
    "    stream_temp.columns = gcms\n",
    "    axes[1, 0].plot(stream_temp, linewidth=0.7, label=scenario)\n",
    "    axes[1, 0].title.set_text('Yearly min 7-day average')\n",
    "    axes[1, 1].plot(stream_temp.mean(axis=1), label=scenario)\n",
    "    axes[1, 1].title.set_text('Yearly min 7-day average, GCM means')   \n",
    "    \n",
    "    z = [x.groupby(pd.Grouper(freq='7d')).mean() for x in stream_temps_corrected[i]]\n",
    "    stream_temp = pd.concat([x.groupby(pd.Grouper(freq='y')).max() for x in z], axis=1)\n",
    "    stream_temp.columns = gcms\n",
    "    axes[2, 0].plot(stream_temp, linewidth=0.7, label=scenario)\n",
    "    axes[2, 0].title.set_text('Yearly max 7-day average')\n",
    "    axes[2, 1].plot(stream_temp.mean(axis=1), label=scenario)\n",
    "    axes[2, 1].title.set_text('Yearly max 7-day average, GCM means')   \n",
    "\n",
    "axes[0, 1].legend(loc='upper right', bbox_to_anchor=(1, 1.5), fancybox=True, ncol=3)\n",
    "plt.ylabel('Temperature (C)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-civilization",
   "metadata": {},
   "source": [
    "## Stream chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "modified-draft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991b9d455da44c17b4f4bd5ba19943c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=4, figsize=(6.5, 5.5))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x[['NH4(gN/m2)_Layer1', 'NH4(gN/m2)_Layer2', 'NH4(gN/m2)_Layer3', 'NH4(gN/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    nh4 = pd.concat(y, axis=1)\n",
    "    nh4.columns = gcms\n",
    "    axes[0, 0].plot(nh4, linewidth=0.7)\n",
    "    axes[0, 1].plot(nh4.mean(axis=1), label=scenario)\n",
    "    axes[0, 0].title.set_text('NH4')\n",
    "    axes[0, 1].title.set_text('NH4, GCM means')\n",
    "\n",
    "    z = [x[['NO3(gN/m2)_Layer1', 'NO3(gN/m2)_Layer2', 'NO3(gN/m2)_Layer3', 'NO3(gN/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    no3 = pd.concat(y, axis=1)\n",
    "    no3.columns = gcms\n",
    "    axes[1, 0].plot(no3, linewidth=0.7)\n",
    "    axes[1, 1].plot(no3.mean(axis=1), label=scenario)\n",
    "    axes[1, 0].title.set_text('NO3')\n",
    "    axes[1, 1].title.set_text('NO3, GCM means')\n",
    "    \n",
    "    z = [x[['DON(gN/m2)_Layer1', 'DON(gN/m2)_Layer2', 'DON(gN/m2)_Layer3', 'DON(gN/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    don = pd.concat(y, axis=1)\n",
    "    don.columns = gcms\n",
    "    axes[2, 0].plot(don, linewidth=0.7)\n",
    "    axes[2, 1].plot(don.mean(axis=1), label=scenario)\n",
    "    axes[2, 0].title.set_text('DON')\n",
    "    axes[2, 1].title.set_text('DON, GCM means')\n",
    "\n",
    "    z = [x[['DOC(gC/m2)_Layer1', 'DOC(gC/m2)_Layer2', 'DOC(gC/m2)_Layer3', 'DOC(gC/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    doc = pd.concat(y, axis=1)\n",
    "    doc.columns = gcms\n",
    "    axes[3, 0].plot(doc, linewidth=0.7) \n",
    "    axes[3, 1].plot(doc.mean(axis=1), label=scenario)\n",
    "    axes[3, 0].title.set_text('DOC')\n",
    "    axes[3, 1].title.set_text('DOC, GCM means')\n",
    "\n",
    "axes[0, 1].legend(loc='upper right', bbox_to_anchor=(1, 1.6), fancybox=True, ncol=3)\n",
    "plt.ylabel('g/m2')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-watson",
   "metadata": {},
   "source": [
    "# Carbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "refined-hacker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591ec441a4084222b53ae0ed2d3648f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=5, figsize=(8, 7))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x['agBiomass_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    ag_biomass_pool = pd.concat(z, axis=1)\n",
    "    ag_biomass_pool.columns = gcms\n",
    "    axes[0, 0].plot(ag_biomass_pool, linewidth=0.7)\n",
    "    axes[0, 1].plot(ag_biomass_pool.mean(axis=1), label=scenario)\n",
    "    axes[0, 0].title.set_text('agBiomass Pool')\n",
    "    axes[0, 1].title.set_text('GCM Means')\n",
    " \n",
    "    z = [x['bgBiomass_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    bg_biomass_pool = pd.concat(z, axis=1)\n",
    "    bg_biomass_pool.columns = gcms\n",
    "    axes[1, 0].plot(bg_biomass_pool, linewidth=0.7)\n",
    "    axes[1, 1].plot(bg_biomass_pool.mean(axis=1), label=scenario)\n",
    "    axes[1, 0].title.set_text('bgBiomass Pool')\n",
    "\n",
    "    z = [x['agLitter_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    ag_litter_pool = pd.concat(z, axis=1)\n",
    "    ag_litter_pool.columns = gcms\n",
    "    axes[2, 0].plot(ag_litter_pool, linewidth=0.7)\n",
    "    axes[2, 1].plot(ag_litter_pool.mean(axis=1), label=scenario)\n",
    "    axes[2, 0].title.set_text('agLitter Pool')\n",
    "\n",
    "    z = [x['bgLitter_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    bg_litter_pool = pd.concat(z, axis=1)\n",
    "    bg_litter_pool.columns = gcms\n",
    "    axes[3, 0].plot(bg_litter_pool, linewidth=0.7) \n",
    "    axes[3, 1].plot(bg_litter_pool.mean(axis=1), label=scenario)\n",
    "    axes[3, 0].title.set_text('agLitter Pool')\n",
    "    \n",
    "    z = [x['Humus_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    humus_pool = pd.concat(z, axis=1)\n",
    "    humus_pool.columns = gcms\n",
    "    axes[4, 0].plot(humus_pool, linewidth=0.7) \n",
    "    axes[4, 1].plot(humus_pool.mean(axis=1), label=scenario)\n",
    "    axes[4, 0].title.set_text('Humus Pool')\n",
    "    \n",
    "axes[0, 1].legend(loc='upper right', bbox_to_anchor=(1, 1.6), fancybox=True, ncol=3)\n",
    "plt.ylabel('gC/m2')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnc_velma",
   "language": "python",
   "name": "tnc_velma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
