{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "differential-monitoring",
   "metadata": {},
   "source": [
    "# Comparing Forest Management Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "twelve-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import __init__\n",
    "import scripts.config as config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import datetime\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from statsmodels.iolib.smpickle import load_pickle\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "improving-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting parameters\n",
    "\n",
    "XSMALL_SIZE = 6\n",
    "SMALL_SIZE = 7\n",
    "MEDIUM_SIZE = 9\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-weapon",
   "metadata": {},
   "source": [
    "This script will visualize the simulation results using all GCMs for all forest management scenarios. All you need to do is include the correct GCM name in the 'gcms' list and scenario names in the 'scenarios' list. These should be identical to the suffixes used in the directories and VELMA XML titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "sixth-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcms = ['canesm2_RCP85', 'ccsm4_RCP85', 'giss_e2_h_RCP85', 'noresm1_m_RCP85']\n",
    "\n",
    "# Import driver data\n",
    "sim_start = pd.to_datetime('01-01-2020')\n",
    "sim_end = pd.to_datetime('12-31-2099')\n",
    "sim_range = pd.date_range(sim_start, sim_end)\n",
    "\n",
    "temp_files = []\n",
    "precip_files = []\n",
    "for gcm in gcms:\n",
    "    temp_file_path = config.velma_data / 'temp' / '{}_{}_{}_temp.csv'.format(gcm, \n",
    "                                                                             sim_start.year % 100,\n",
    "                                                                             sim_end.year % 100)\n",
    "    temp_file = pd.read_csv(temp_file_path, names=['temp'])\n",
    "    temp_file.index = sim_range\n",
    "    temp_files.append(temp_file)\n",
    "    \n",
    "    precip_file_path = config.velma_data / 'precip' / '{}_{}_{}_ppt.csv'.format(gcm, \n",
    "                                                                                sim_start.year % 100, \n",
    "                                                                                sim_end.year % 100)\n",
    "    precip_file = pd.read_csv(precip_file_path, names=['precip'])\n",
    "    precip_file.index = sim_range\n",
    "    precip_files.append(precip_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "executive-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import results files\n",
    "\n",
    "# scenarios = ['baseline', 'baseline_water_balance_errors']\n",
    "# scenarios = ['baseline', 'ind35yr', 'activeall', 'historical']\n",
    "scenarios = ['baseline', 'ind35yr', 'activeall']\n",
    "\n",
    "dailies = []\n",
    "annuals = []\n",
    "\n",
    "# Import daily and annual results\n",
    "# Results in nested lists ([x][y], for x management scenarios and y GCMs)\n",
    "for scenario in scenarios:\n",
    "    dailies_scenario = []\n",
    "    annuals_scenario = []\n",
    "    scenario_dir = config.velma_data.parents[1] / 'results' / scenario\n",
    "    dirs = os.listdir(scenario_dir)\n",
    "    for directory in dirs:\n",
    "        \n",
    "        results_dir = scenario_dir / 'ellsworth_{}_{}_{}_{}'.format(scenario,\n",
    "                                                                    sim_start.year % 100,\n",
    "                                                                    sim_end.year % 100,\n",
    "                                                                    gcm)\n",
    "\n",
    "        daily_results = pd.read_csv(results_dir / 'DailyResults.csv')\n",
    "\n",
    "        # Format datetime\n",
    "        jday_pad = daily_results['Day'].apply(lambda x: str(x).zfill(3))\n",
    "        str_year = daily_results['Year'].apply(lambda x: str(x))\n",
    "        rng = pd.to_datetime((str_year + jday_pad), format='%Y%j')\n",
    "        daily_results.index = rng\n",
    "        dailies_scenario.append(daily_results)\n",
    "\n",
    "#         annual_results = pd.read_csv(results_dir / 'AnnualResults.csv')\n",
    "#         annuals_scenario.append(annual_results)\n",
    "        \n",
    "    dailies.append(dailies_scenario)\n",
    "#     annuals.append(annuals_scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-bosnia",
   "metadata": {},
   "source": [
    "## Runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "international-blackjack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599f66cce8d146e38f6df2d7a3aba8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Daily runoff time series (all years together)\n",
    "\n",
    "# # Check that all dfs have same number of columns\n",
    "# all([len(dailies[0].columns.intersection(df.columns)) \n",
    "#       == dailies[0].shape[1] for df in dailies])\n",
    "\n",
    "# colors = sns.color_palette('tab10', len(sims))\n",
    "\n",
    "fig, axes = plt.subplots(ncols=1, nrows=4, figsize=(5, 6))\n",
    "\n",
    "z = [x.groupby(pd.Grouper(freq='w')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[0]]\n",
    "yearly_7day_min = pd.concat([x.groupby(pd.Grouper(freq='y')).min() for x in z], axis=1)\n",
    "yearly_7day_min.columns = gcms\n",
    "axes[0].plot(yearly_7day_min, linewidth=0.8)\n",
    "axes[0].title.set_text('Yearly min 7-day flow')\n",
    "\n",
    "yearly_7day_max = pd.concat([x.groupby(pd.Grouper(freq='y')).max() for x in z], axis=1)\n",
    "yearly_7day_max.columns = gcms\n",
    "axes[1].plot(yearly_7day_max, linewidth=0.8)\n",
    "axes[1].title.set_text('Yearly max 7-day flow')\n",
    "\n",
    "z = [x.groupby(pd.Grouper(freq='y')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[0]]\n",
    "yearly_sum = pd.concat(z, axis=1)\n",
    "yearly_sum.columns = gcms\n",
    "axes[2].plot(yearly_sum, linewidth=0.8)\n",
    "axes[2].title.set_text('Yearly sum flow')\n",
    "\n",
    "z = [x.groupby(pd.Grouper(freq='y')).mean()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[0]]\n",
    "yearly_mean = pd.concat(z, axis=1)\n",
    "yearly_mean.columns = gcms\n",
    "axes[3].plot(yearly_mean, linewidth=0.8)\n",
    "axes[3].title.set_text('Yearly mean flow')\n",
    "    \n",
    "plt.ylabel('Runoff (mm)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "italian-burst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bf392788be433fb8a4b417491e33e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Averaged GCM flows for all management scenarios\n",
    "fig, axes = plt.subplots(ncols=2, nrows=4, figsize=(7, 6))\n",
    "combos = list(itertools.product(gcms, scenarios))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x.groupby(pd.Grouper(freq='7d')).mean()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[i]]\n",
    "    yearly_7day_min = pd.concat([x.groupby(pd.Grouper(freq='y')).min() for x in z], axis=1)\n",
    "    yearly_7day_min.columns = gcms\n",
    "    axes[0, 0].plot(yearly_7day_min, linewidth=0.7)\n",
    "    axes[0, 1].plot(yearly_7day_min.mean(axis=1), linewidth=0.9, label=scenario)\n",
    "    axes[0, 0].title.set_text('Yearly min 7-day flow')\n",
    "    axes[0, 1].title.set_text('Yearly min, GCM means')\n",
    "    axes[0, 0].set_ylim(0, 2)\n",
    "    axes[0, 1].set_ylim(0, 2)\n",
    "\n",
    "    yearly_7day_max = pd.concat([x.groupby(pd.Grouper(freq='y')).max() for x in z], axis=1)\n",
    "    yearly_7day_max.columns = gcms\n",
    "    axes[1, 0].plot(yearly_7day_max, linewidth=0.7)\n",
    "    axes[1, 1].plot(yearly_7day_max.mean(axis=1), linewidth=0.9, label=scenario)\n",
    "    axes[1, 0].title.set_text('Yearly max 7-day flow')\n",
    "    axes[1, 1].title.set_text('Yearly max, GCM means')\n",
    "\n",
    "    z = [x.groupby(pd.Grouper(freq='y')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[i]]\n",
    "    yearly_sum = pd.concat(z, axis=1)\n",
    "    yearly_sum.columns = gcms\n",
    "    axes[2, 0].plot(yearly_sum, linewidth=0.7)\n",
    "    axes[2, 1].plot(yearly_sum.mean(axis=1), linewidth=0.9, label=scenario)\n",
    "    axes[2, 0].title.set_text('Yearly sum flow')\n",
    "    axes[2, 1].title.set_text('Yearly sum, GCM means')\n",
    "\n",
    "    z = [x.groupby(pd.Grouper(freq='y')).mean()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[i]]\n",
    "    yearly_mean = pd.concat(z, axis=1)\n",
    "    yearly_mean.columns = gcms\n",
    "    axes[3, 0].plot(yearly_mean, linewidth=0.7) \n",
    "    axes[3, 1].plot(yearly_mean.mean(axis=1), linewidth=0.9, label=scenario)\n",
    "    axes[3, 0].title.set_text('Yearly mean flow')\n",
    "    axes[3, 1].title.set_text('Yearly mean, GCM means')\n",
    "\n",
    "axes[0, 1].legend(loc='upper right', bbox_to_anchor=(1, 1.5), fancybox=True, ncol=3)\n",
    "plt.ylabel('Runoff (mm)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-patent",
   "metadata": {},
   "source": [
    "## Stream temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "august-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cell writer files\n",
    "\n",
    "cell_results = []\n",
    "for scenario in scenarios:\n",
    "    cell_results_scenario = []\n",
    "    scenario_dir = config.velma_data.parents[1] / 'results' / scenario\n",
    "    dirs = os.listdir(scenario_dir)\n",
    "    for directory in dirs:\n",
    "        cell_paths = []\n",
    "        results_dir = scenario_dir / 'ellsworth_{}_{}_{}_{}'.format(scenario,\n",
    "                                                                    sim_start.year % 100,\n",
    "                                                                    sim_end.year % 100,\n",
    "                                                                    gcm)\n",
    "        for file in os.listdir(results_dir):\n",
    "            if file.startswith('Cell_'):\n",
    "                cell_paths.append(file)\n",
    "        \n",
    "        nodes = []\n",
    "        for path in cell_paths:\n",
    "            nodes.append(path.split('_')[-1])\n",
    "        \n",
    "        cell_paths_sorted = [x for _,x in natsorted(zip(nodes,cell_paths))]\n",
    "        \n",
    "        for path in [cell_paths_sorted[0]]:  # Only need the first cell, which is the Ellsworth mouth/outlet\n",
    "            cell_result = pd.read_csv(results_dir / path)\n",
    "            jday_pad = cell_result['Jday'].apply(lambda x: str(x).zfill(3))\n",
    "            str_year = cell_result['Year'].apply(lambda x: str(x))\n",
    "            cell_result['date'] = str_year + jday_pad\n",
    "            rng = pd.to_datetime(cell_result['date'], format='%Y%j')\n",
    "            cell_result.index = rng\n",
    "            cell_results_scenario.append(cell_result)\n",
    "    \n",
    "    cell_results.append(cell_results_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "technical-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct VELMA stream temperature seasonal bias using pre-trained regression model\n",
    "# *** Not sure if this correction is still valid considering the non-linear seasonal changes of the climate projections ***\n",
    "\n",
    "olsmodel = load_pickle(config.data_path.parents[0] / 'models' / 'stream_temp_correction_ols.pickle')\n",
    "\n",
    "stream_temps_corrected = []\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    stream_temps_scenario = []\n",
    "    for j, gcm in enumerate(gcms):\n",
    "        z = cell_results[i][j]['Water_Surface_Temperature(degrees_C)']\n",
    "        \n",
    "        day = 24 * 60 * 60\n",
    "        year = 365.2425 * day\n",
    "        timestamp_secs = pd.to_datetime(z.index)\n",
    "        timestamp_secs = timestamp_secs.map(datetime.datetime.timestamp)\n",
    "        year_cos = np.cos(timestamp_secs * (2 * np.pi / year))\n",
    "        year_sin = np.sin(timestamp_secs * (2 * np.pi / year))\n",
    "\n",
    "        y = pd.DataFrame(data=np.column_stack([z, year_cos, year_sin]), columns=['temp', 'year_cos', 'year_sin'])\n",
    "        y.index = z.index\n",
    "        y['air_temp_3day_avg'] = y['temp'].rolling(3, min_periods=0).mean()\n",
    "\n",
    "        y = sm.add_constant(y)\n",
    "        y['streamtemp_corrected'] = olsmodel.predict(y)\n",
    "        \n",
    "        stream_temps_scenario.append(y['streamtemp_corrected'])\n",
    "    \n",
    "    stream_temps_corrected.append(stream_temps_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "judicial-rough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b0ecc7c471447599bf34a2b53a40e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "fig, axes = plt.subplots(ncols=2, nrows=3, figsize=(7, 5))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x for x in stream_temps_corrected[i]]\n",
    "    stream_temp = pd.concat([x.groupby(pd.Grouper(freq='y')).mean() for x in z], axis=1)\n",
    "    stream_temp.columns = gcms\n",
    "    axes[0, 0].plot(stream_temp, linewidth=0.7, label=scenario)\n",
    "    axes[0, 0].title.set_text('Yearly mean')\n",
    "    axes[0, 1].plot(stream_temp.mean(axis=1), label=scenario)\n",
    "    axes[0, 1].title.set_text('Yearly mean, GCM means')\n",
    "    \n",
    "    z = [x.groupby(pd.Grouper(freq='7d')).mean() for x in stream_temps_corrected[i]]\n",
    "    stream_temp = pd.concat([x.groupby(pd.Grouper(freq='y')).min() for x in z], axis=1)\n",
    "    stream_temp.columns = gcms\n",
    "    axes[1, 0].plot(stream_temp, linewidth=0.7, label=scenario)\n",
    "    axes[1, 0].title.set_text('Yearly min 7-day average')\n",
    "    axes[1, 1].plot(stream_temp.mean(axis=1), label=scenario)\n",
    "    axes[1, 1].title.set_text('Yearly min 7-day average, GCM means')   \n",
    "    \n",
    "    z = [x.groupby(pd.Grouper(freq='7d')).mean() for x in stream_temps_corrected[i]]\n",
    "    stream_temp = pd.concat([x.groupby(pd.Grouper(freq='y')).max() for x in z], axis=1)\n",
    "    stream_temp.columns = gcms\n",
    "    axes[2, 0].plot(stream_temp, linewidth=0.7, label=scenario)\n",
    "    axes[2, 0].title.set_text('Yearly max 7-day average')\n",
    "    axes[2, 1].plot(stream_temp.mean(axis=1), label=scenario)\n",
    "    axes[2, 1].title.set_text('Yearly max 7-day average, GCM means')   \n",
    "\n",
    "axes[0, 1].legend(loc='upper right', bbox_to_anchor=(1, 1.5), fancybox=True, ncol=3)\n",
    "plt.ylabel('Temperature (C)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-support",
   "metadata": {},
   "source": [
    "## Stream chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "adverse-health",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e11b17a0b94ec3814b33b08937026e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=4, figsize=(6.5, 5.5))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x[['NH4(gN/m2)_Layer1', 'NH4(gN/m2)_Layer2', 'NH4(gN/m2)_Layer3', 'NH4(gN/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    nh4 = pd.concat(y, axis=1)\n",
    "    nh4.columns = gcms\n",
    "    axes[0, 0].plot(nh4, linewidth=0.7)\n",
    "    axes[0, 1].plot(nh4.mean(axis=1), label=scenario)\n",
    "    axes[0, 0].title.set_text('NH4')\n",
    "    axes[0, 1].title.set_text('NH4, GCM means')\n",
    "\n",
    "    z = [x[['NO3(gN/m2)_Layer1', 'NO3(gN/m2)_Layer2', 'NO3(gN/m2)_Layer3', 'NO3(gN/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    no3 = pd.concat(y, axis=1)\n",
    "    no3.columns = gcms\n",
    "    axes[1, 0].plot(no3, linewidth=0.7)\n",
    "    axes[1, 1].plot(no3.mean(axis=1), label=scenario)\n",
    "    axes[1, 0].title.set_text('NO3')\n",
    "    axes[1, 1].title.set_text('NO3, GCM means')\n",
    "    \n",
    "    z = [x[['DON(gN/m2)_Layer1', 'DON(gN/m2)_Layer2', 'DON(gN/m2)_Layer3', 'DON(gN/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    don = pd.concat(y, axis=1)\n",
    "    don.columns = gcms\n",
    "    axes[2, 0].plot(don, linewidth=0.7)\n",
    "    axes[2, 1].plot(don.mean(axis=1), label=scenario)\n",
    "    axes[2, 0].title.set_text('DON')\n",
    "    axes[2, 1].title.set_text('DON, GCM means')\n",
    "\n",
    "    z = [x[['DOC(gC/m2)_Layer1', 'DOC(gC/m2)_Layer2', 'DOC(gC/m2)_Layer3', 'DOC(gC/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    doc = pd.concat(y, axis=1)\n",
    "    doc.columns = gcms\n",
    "    axes[3, 0].plot(doc, linewidth=0.7) \n",
    "    axes[3, 1].plot(doc.mean(axis=1), label=scenario)\n",
    "    axes[3, 0].title.set_text('DOC')\n",
    "    axes[3, 1].title.set_text('DOC, GCM means')\n",
    "\n",
    "axes[0, 1].legend(loc='upper right', bbox_to_anchor=(1, 1.6), fancybox=True, ncol=3)\n",
    "plt.ylabel('g/m2')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-reality",
   "metadata": {},
   "source": [
    "## Carbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "sweet-knitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a86ac6b09b547d299eadca831140586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=5, figsize=(8, 7))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x['agBiomass_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    ag_biomass_pool = pd.concat(z, axis=1)\n",
    "    ag_biomass_pool.columns = gcms\n",
    "    axes[0, 0].plot(ag_biomass_pool, linewidth=0.7)\n",
    "    axes[0, 1].plot(ag_biomass_pool.mean(axis=1), label=scenario)\n",
    "    axes[0, 0].title.set_text('agBiomass Pool')\n",
    "    axes[0, 1].title.set_text('GCM Means')\n",
    " \n",
    "    z = [x['bgBiomass_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    bg_biomass_pool = pd.concat(z, axis=1)\n",
    "    bg_biomass_pool.columns = gcms\n",
    "    axes[1, 0].plot(bg_biomass_pool, linewidth=0.7)\n",
    "    axes[1, 1].plot(bg_biomass_pool.mean(axis=1), label=scenario)\n",
    "    axes[1, 0].title.set_text('bgBiomass Pool')\n",
    "\n",
    "    z = [x['agLitter_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    ag_litter_pool = pd.concat(z, axis=1)\n",
    "    ag_litter_pool.columns = gcms\n",
    "    axes[2, 0].plot(ag_litter_pool, linewidth=0.7)\n",
    "    axes[2, 1].plot(ag_litter_pool.mean(axis=1), label=scenario)\n",
    "    axes[2, 0].title.set_text('agLitter Pool')\n",
    "\n",
    "    z = [x['bgLitter_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    bg_litter_pool = pd.concat(z, axis=1)\n",
    "    bg_litter_pool.columns = gcms\n",
    "    axes[3, 0].plot(bg_litter_pool, linewidth=0.7) \n",
    "    axes[3, 1].plot(bg_litter_pool.mean(axis=1), label=scenario)\n",
    "    axes[3, 0].title.set_text('agLitter Pool')\n",
    "    \n",
    "    z = [x['Humus_Pool(gC/m2)_Delineated_Average'] for x in dailies[i]]\n",
    "    humus_pool = pd.concat(z, axis=1)\n",
    "    humus_pool.columns = gcms\n",
    "    axes[4, 0].plot(humus_pool, linewidth=0.7) \n",
    "    axes[4, 1].plot(humus_pool.mean(axis=1), label=scenario)\n",
    "    axes[4, 0].title.set_text('Humus Pool')\n",
    "    \n",
    "axes[0, 1].legend(loc='upper right', bbox_to_anchor=(1, 1.6), fancybox=True, ncol=3)\n",
    "plt.ylabel('gC/m2')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "theoretical-better",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 4 4 4 4 4 4 4 4 4 4 4 4 4 4 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ecf623c0e65e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvelma_data\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'soil'\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'MapunitRaster_10m.asc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tnc_velma\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[1;31m# converting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tnc_velma\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(chunk_size)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tnc_velma\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tnc_velma\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    742\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    745\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlongdouble\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlongdouble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 4 4 4 4 4 4 4 4 4 4 4 4 4 4 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10'"
     ]
    }
   ],
   "source": [
    "soil = np.loadtxt(config.velma_data / 'soil' / 'MapunitRaster_10m.asc', dtype='int', delimiter=',', skiprows=6)\n",
    "plt.figure()\n",
    "plt.imshow(soil)\n",
    "np.unique(soil)\n",
    "soil.dtype\n",
    "# soil_int = soil.astype('int')\n",
    "# soil_int\n",
    "\n",
    "from soil.soil_merger import readHeader\n",
    "header = readHeader(config.velma_data / 'soil' / 'MapunitRaster_10m.asc')\n",
    "outfile = config.velma_data / 'soil' / 'MapunitRaster_10m.asc'\n",
    "f = open(outfile, 'w')\n",
    "f.write(header)\n",
    "np.savetxt(f, soil_int.astype(int), fmt='%i')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-condition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnc_velma",
   "language": "python",
   "name": "tnc_velma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
