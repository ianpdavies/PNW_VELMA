{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enormous-syndication",
   "metadata": {},
   "source": [
    "# Comparing Forest Management Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import __init__\n",
    "import scripts.config as config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import datetime\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from statsmodels.iolib.smpickle import load_pickle\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting parameters\n",
    "\n",
    "XSMALL_SIZE = 6\n",
    "SMALL_SIZE = 7\n",
    "MEDIUM_SIZE = 9\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-triangle",
   "metadata": {},
   "source": [
    "This script will visualize the simulation results using all GCMs for all forest management scenarios. All you need to do is include the correct GCM name in the 'gcms' list and scenario names in the 'scenarios' list. These should be identical to the suffixes used in the directories and VELMA XML titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcms = ['canesm2_RCP85', 'ccsm4_RCP85', 'giss_e2_h_RCP85', 'noresm1_m_RCP85']\n",
    "\n",
    "# Import driver data\n",
    "sim_start = pd.to_datetime('01-01-2020')\n",
    "sim_end = pd.to_datetime('12-31-2099')\n",
    "sim_range = pd.date_range(sim_start, sim_end)\n",
    "\n",
    "temp_files = []\n",
    "precip_files = []\n",
    "for gcm in gcms:\n",
    "    temp_file_path = config.velma_data / 'temp' / '{}_{}_{}_temp.csv'.format(gcm, \n",
    "                                                                             sim_start.year % 100,\n",
    "                                                                             sim_end.year % 100)\n",
    "    temp_file = pd.read_csv(temp_file_path, names=['temp'])\n",
    "    temp_file.index = sim_range\n",
    "    temp_files.append(temp_file)\n",
    "    \n",
    "    precip_file_path = config.velma_data / 'precip' / '{}_{}_{}_ppt.csv'.format(gcm, \n",
    "                                                                                sim_start.year % 100, \n",
    "                                                                                sim_end.year % 100)\n",
    "    precip_file = pd.read_csv(precip_file_path, names=['precip'])\n",
    "    precip_file.index = sim_range\n",
    "    precip_files.append(precip_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import results files\n",
    "\n",
    "scenarios = ['baseline', 'ind35yr']\n",
    "\n",
    "dailies = []\n",
    "annuals = []\n",
    "\n",
    "# Import daily and annual results\n",
    "# Results in nested lists ([x][y], for x management scenarios and y GCMs)\n",
    "for scenario in scenarios:\n",
    "    dailies_scenario = []\n",
    "    annuals_scenario = []\n",
    "    scenario_dir = config.velma_data.parents[1] / 'results' / scenario\n",
    "    for gcm in gcms:\n",
    "        results_dir = scenario_dir / 'ellsworth_{}_{}_{}_{}'.format(scenario,\n",
    "                                                                    sim_start.year % 100,\n",
    "                                                                    sim_end.year % 100,\n",
    "                                                                    gcm)\n",
    "\n",
    "        daily_results = pd.read_csv(results_dir / 'DailyResults.csv')\n",
    "\n",
    "        # Format datetime\n",
    "        jday_pad = daily_results['Day'].apply(lambda x: str(x).zfill(3))\n",
    "        str_year = daily_results['Year'].apply(lambda x: str(x))\n",
    "        rng = pd.to_datetime((str_year + jday_pad), format='%Y%j')\n",
    "        daily_results.index = rng\n",
    "        dailies_scenario.append(daily_results)\n",
    "\n",
    "        annual_results = pd.read_csv(results_dir / 'AnnualResults.csv')\n",
    "        annuals_scenario.append(annual_results)\n",
    "        \n",
    "    dailies.append(dailies_scenario)\n",
    "    annuals.append(annuals_scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-kitchen",
   "metadata": {},
   "source": [
    "## Runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-drain",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Daily runoff time series (all years together)\n",
    "\n",
    "# # Check that all dfs have same number of columns\n",
    "# all([len(dailies[0].columns.intersection(df.columns)) \n",
    "#       == dailies[0].shape[1] for df in dailies])\n",
    "\n",
    "# colors = sns.color_palette('tab10', len(sims))\n",
    "\n",
    "fig, axes = plt.subplots(ncols=1, nrows=4, figsize=(9, 10))\n",
    "\n",
    "z = [x.groupby(pd.Grouper(freq='w')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[0]]\n",
    "yearly_7day_min = pd.concat([x.groupby(pd.Grouper(freq='y')).min() for x in z], axis=1)\n",
    "yearly_7day_min.columns = gcms\n",
    "axes[0].plot(yearly_7day_min)\n",
    "axes[0].title.set_text('Yearly min 7-day flow')\n",
    "\n",
    "yearly_7day_max = pd.concat([x.groupby(pd.Grouper(freq='y')).max() for x in z], axis=1)\n",
    "yearly_7day_max.columns = gcms\n",
    "axes[1].plot(yearly_7day_max)\n",
    "axes[1].title.set_text('Yearly max 7-day flow')\n",
    "\n",
    "z = [x.groupby(pd.Grouper(freq='y')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[0]]\n",
    "yearly_sum = pd.concat(z, axis=1)\n",
    "yearly_sum.columns = gcms\n",
    "axes[2].plot(yearly_sum)\n",
    "axes[2].title.set_text('Yearly sum flow')\n",
    "\n",
    "z = [x.groupby(pd.Grouper(freq='y')).mean()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[0]]\n",
    "yearly_mean = pd.concat(z, axis=1)\n",
    "yearly_mean.columns = gcms\n",
    "axes[3].plot(yearly_mean)\n",
    "axes[3].title.set_text('Yearly mean flow')\n",
    "    \n",
    "plt.ylabel('Runoff (mm)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged GCM flows for all management scenarios\n",
    "fig, axes = plt.subplots(ncols=2, nrows=4, figsize=(8, 7))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x.groupby(pd.Grouper(freq='w')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[i]]\n",
    "    yearly_7day_min = pd.concat([x.groupby(pd.Grouper(freq='y')).min() for x in z], axis=1)\n",
    "    yearly_7day_min.columns = gcms\n",
    "    axes[0, 0].plot(yearly_7day_min, linewidth=0.7)\n",
    "    axes[0, 1].plot(yearly_7day_min.mean(axis=1))\n",
    "    axes[0, 0].title.set_text('Yearly min 7-day flow')\n",
    "\n",
    "    yearly_7day_max = pd.concat([x.groupby(pd.Grouper(freq='y')).max() for x in z], axis=1)\n",
    "    yearly_7day_max.columns = gcms\n",
    "    axes[1, 0].plot(yearly_7day_max, linewidth=0.7)\n",
    "    axes[1, 1].plot(yearly_7day_max.mean(axis=1))\n",
    "    axes[1, 0].title.set_text('Yearly max 7-day flow')\n",
    "\n",
    "    z = [x.groupby(pd.Grouper(freq='y')).sum()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[i]]\n",
    "    yearly_sum = pd.concat(z, axis=1)\n",
    "    yearly_sum.columns = gcms\n",
    "    axes[2, 0].plot(yearly_sum, linewidth=0.7)\n",
    "    axes[2, 1].plot(yearly_sum.mean(axis=1))\n",
    "    axes[2, 0].title.set_text('Yearly sum flow')\n",
    "\n",
    "    z = [x.groupby(pd.Grouper(freq='y')).mean()['Runoff_All(mm/day)_Delineated_Average'] for x in dailies[i]]\n",
    "    yearly_mean = pd.concat(z, axis=1)\n",
    "    yearly_mean.columns = gcms\n",
    "    axes[3, 0].plot(yearly_mean, linewidth=0.7) \n",
    "    axes[3, 1].plot(yearly_mean.mean(axis=1))\n",
    "    axes[3, 0].title.set_text('Yearly mean flow')\n",
    "\n",
    "plt.ylabel('Runoff (mm)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-innocent",
   "metadata": {},
   "source": [
    "## Stream temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import results files\n",
    "\n",
    "scenarios = ['baseline', 'ind35yr']\n",
    "\n",
    "dailies = []\n",
    "annuals = []\n",
    "\n",
    "# Import daily and annual results\n",
    "# Results in nested lists ([x][y], for x management scenarios and y GCMs)\n",
    "for scenario in scenarios:\n",
    "    dailies_scenario = []\n",
    "    annuals_scenario = []\n",
    "    scenario_dir = config.velma_data.parents[1] / 'results' / scenario\n",
    "    for gcm in gcms:\n",
    "        results_dir = scenario_dir / 'ellsworth_{}_{}_{}_{}'.format(scenario,\n",
    "                                                                    sim_start.year % 100,\n",
    "                                                                    sim_end.year % 100,\n",
    "                                                                    gcm)\n",
    "\n",
    "        daily_results = pd.read_csv(results_dir / 'DailyResults.csv')\n",
    "\n",
    "        # Format datetime\n",
    "        jday_pad = daily_results['Day'].apply(lambda x: str(x).zfill(3))\n",
    "        str_year = daily_results['Year'].apply(lambda x: str(x))\n",
    "        rng = pd.to_datetime((str_year + jday_pad), format='%Y%j')\n",
    "        daily_results.index = rng\n",
    "        dailies_scenario.append(daily_results)\n",
    "\n",
    "        annual_results = pd.read_csv(results_dir / 'AnnualResults.csv')\n",
    "        annuals_scenario.append(annual_results)\n",
    "        \n",
    "    dailies.append(dailies_scenario)\n",
    "    annuals.append(annuals_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cell writer files\n",
    "\n",
    "cell_results = []\n",
    "for scenario in scenarios:\n",
    "    cell_results_scenario = []\n",
    "    scenario_dir = config.velma_data.parents[1] / 'results' / scenario\n",
    "    for gcm in gcms:\n",
    "        cell_paths = []\n",
    "        results_dir = scenario_dir / 'ellsworth_{}_{}_{}_{}'.format(scenario,\n",
    "                                                                    sim_start.year % 100,\n",
    "                                                                    sim_end.year % 100,\n",
    "                                                                    gcm)\n",
    "        for file in os.listdir(results_dir):\n",
    "            if file.startswith('Cell_'):\n",
    "                cell_paths.append(file)\n",
    "        \n",
    "        nodes = []\n",
    "        for path in cell_paths:\n",
    "            nodes.append(path.split('_')[-1])\n",
    "        \n",
    "        cell_paths_sorted = [x for _,x in natsorted(zip(nodes,cell_paths))]\n",
    "        \n",
    "        for path in [cell_paths_sorted[0]]:  # Only need the first cell, which is the Ellsworth mouth/outlet\n",
    "            cell_result = pd.read_csv(results_dir / path)\n",
    "            jday_pad = cell_result['Jday'].apply(lambda x: str(x).zfill(3))\n",
    "            str_year = cell_result['Year'].apply(lambda x: str(x))\n",
    "            cell_result['date'] = str_year + jday_pad\n",
    "            rng = pd.to_datetime(cell_result['date'], format='%Y%j')\n",
    "            cell_result.index = rng\n",
    "            cell_results_scenario.append(cell_result)\n",
    "    \n",
    "    cell_results.append(cell_results_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct VELMA stream temperature seasonal bias using pre-trained regression model\n",
    "# *** Not sure if this correction is still valid considering the non-linear seasonal changes of the climate projections ***\n",
    "\n",
    "olsmodel = load_pickle(config.data_path.parents[0] / 'models' / 'stream_temp_correction_ols.pickle')\n",
    "\n",
    "stream_temps_corrected = []\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    stream_temps_scenario = []\n",
    "    for j, gcm in enumerate(gcms):\n",
    "        z = cell_results[i][j]['Water_Surface_Temperature(degrees_C)']\n",
    "        \n",
    "        day = 24 * 60 * 60\n",
    "        year = 365.2425 * day\n",
    "        timestamp_secs = pd.to_datetime(z.index)\n",
    "        timestamp_secs = timestamp_secs.map(datetime.datetime.timestamp)\n",
    "        year_cos = np.cos(timestamp_secs * (2 * np.pi / year))\n",
    "        year_sin = np.sin(timestamp_secs * (2 * np.pi / year))\n",
    "\n",
    "        y = pd.DataFrame(data=np.column_stack([z, year_cos, year_sin]), columns=['temp', 'year_cos', 'year_sin'])\n",
    "        y.index = z.index\n",
    "        y['air_temp_3day_avg'] = y['temp'].rolling(3, min_periods=0).mean()\n",
    "\n",
    "        y = sm.add_constant(y)\n",
    "        y['streamtemp_corrected'] = olsmodel.predict(y)\n",
    "        \n",
    "        stream_temps_scenario.append(y['streamtemp_corrected'])\n",
    "    \n",
    "    stream_temps_corrected.append(stream_temps_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=3, figsize=(8, 6))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x for x in stream_temps_corrected[i]]\n",
    "    stream_temp = pd.concat([x.groupby(pd.Grouper(freq='y')).mean() for x in z], axis=1)\n",
    "    stream_temp.columns = gcms\n",
    "    axes[0, 0].plot(stream_temp, linewidth=0.7)\n",
    "    axes[0, 0].title.set_text('Yearly mean')\n",
    "    axes[0, 1].plot(stream_temp.mean(axis=1))\n",
    "    axes[0, 1].title.set_text('GCM Means')\n",
    "    \n",
    "    z = [x.groupby(pd.Grouper(freq='W')).mean() for x in stream_temps_corrected[i]]\n",
    "    stream_temp = pd.concat([x.groupby(pd.Grouper(freq='y')).min() for x in z], axis=1)\n",
    "    stream_temp.columns = gcms\n",
    "    axes[1, 0].plot(stream_temp, linewidth=0.7)\n",
    "    axes[1, 0].title.set_text('Yearly min 7-day average')\n",
    "    axes[1, 1].plot(stream_temp.mean(axis=1))\n",
    "    axes[1, 1].title.set_text('GCM Means')   \n",
    "    \n",
    "    z = [x.groupby(pd.Grouper(freq='W')).mean() for x in stream_temps_corrected[i]]\n",
    "    stream_temp = pd.concat([x.groupby(pd.Grouper(freq='y')).max() for x in z], axis=1)\n",
    "    stream_temp.columns = gcms\n",
    "    axes[2, 0].plot(stream_temp, linewidth=0.7)\n",
    "    axes[2, 0].title.set_text('Yearly max 7-day average')\n",
    "    axes[2, 1].plot(stream_temp.mean(axis=1))\n",
    "    axes[2, 1].title.set_text('GCM Means')   \n",
    "    \n",
    " \n",
    "\n",
    "plt.ylabel('Temperature (C)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-board",
   "metadata": {},
   "source": [
    "## Stream chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-spanking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=4, figsize=(8, 7))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x[['NH4(gN/m2)_Layer1', 'NH4(gN/m2)_Layer2', 'NH4(gN/m2)_Layer3', 'NH4(gN/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    nh4 = pd.concat(y, axis=1)\n",
    "    nh4.columns = gcms\n",
    "    axes[0, 0].plot(nh4, linewidth=0.7)\n",
    "    axes[0, 1].plot(nh4.mean(axis=1))\n",
    "    axes[0, 0].title.set_text('NH4')\n",
    "    axes[0, 1].title.set_text('GCM Means')\n",
    "\n",
    "    z = [x[['NO3(gN/m2)_Layer1', 'NO3(gN/m2)_Layer2', 'NO3(gN/m2)_Layer3', 'NO3(gN/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    no3 = pd.concat(y, axis=1)\n",
    "    no3.columns = gcms\n",
    "    axes[1, 0].plot(no3, linewidth=0.7)\n",
    "    axes[1, 1].plot(no3.mean(axis=1))\n",
    "    axes[1, 0].title.set_text('NO3')\n",
    "\n",
    "    z = [x[['DON(gN/m2)_Layer1', 'DON(gN/m2)_Layer2', 'DON(gN/m2)_Layer3', 'DON(gN/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    don = pd.concat(y, axis=1)\n",
    "    don.columns = gcms\n",
    "    axes[2, 0].plot(don, linewidth=0.7)\n",
    "    axes[2, 1].plot(don.mean(axis=1))\n",
    "    axes[2, 0].title.set_text('DON')\n",
    "\n",
    "    z = [x[['DOC(gC/m2)_Layer1', 'DOC(gC/m2)_Layer2', 'DOC(gC/m2)_Layer3', 'DOC(gC/m2)_Layer4']].sum(axis=1) for x in cell_results[i]]\n",
    "    y = [x.groupby(pd.Grouper(freq='y')).mean() for x in z]\n",
    "    doc = pd.concat(y, axis=1)\n",
    "    doc.columns = gcms\n",
    "    axes[3, 0].plot(doc, linewidth=0.7) \n",
    "    axes[3, 1].plot(doc.mean(axis=1))\n",
    "    axes[3, 0].title.set_text('DOC')\n",
    "    \n",
    "\n",
    "plt.ylabel('g/m2')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-graphics",
   "metadata": {},
   "source": [
    "## Carbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put carbon results into separate nested list\n",
    "c_columns = [str(x) for x in annuals[0][0].columns[32:58]]\n",
    "c_columns.insert(0, 'Annual_Result')\n",
    "\n",
    "c_results = []\n",
    "c_results_final = []\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    c_results_scenario = []\n",
    "    c_results_final_scenario = []\n",
    "    for j, gcm in enumerate(gcms):\n",
    "        c = annuals[i][j][c_columns]\n",
    "        c.index = annuals[i][j]['Year']\n",
    "        c_final = c[c['Annual_Result'] == 'FINAL_VALUE'].drop(['Annual_Result'], axis=1)\n",
    "        c_results_scenario.append(c)\n",
    "        c_results_final_scenario.append(c_final)\n",
    "    c_results.append(c_results_scenario)\n",
    "    c_results_final.append(c_results_final_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=5, figsize=(8, 7))\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    z = [x['agBiomass_Pool(gC/m2)_Delineated_Average'] for x in c_results_final[i]]\n",
    "    ag_biomass_pool = pd.concat(z, axis=1)\n",
    "    ag_biomass_pool.columns = gcms\n",
    "    axes[0, 0].plot(ag_biomass_pool, linewidth=0.7)\n",
    "    axes[0, 1].plot(ag_biomass_pool.mean(axis=1))\n",
    "    axes[0, 0].title.set_text('agBiomass Pool')\n",
    "    axes[0, 1].title.set_text('GCM Means')\n",
    " \n",
    "    z = [x['bgBiomass_Pool(gC/m2)_Delineated_Average'] for x in c_results_final[i]]\n",
    "    bg_biomass_pool = pd.concat(z, axis=1)\n",
    "    bg_biomass_pool.columns = gcms\n",
    "    axes[1, 0].plot(bg_biomass_pool, linewidth=0.7)\n",
    "    axes[1, 1].plot(bg_biomass_pool.mean(axis=1))\n",
    "    axes[1, 0].title.set_text('bgBiomass Pool')\n",
    "\n",
    "    z = [x['agLitter_Pool(gC/m2)_Delineated_Average'] for x in c_results_final[i]]\n",
    "    ag_litter_pool = pd.concat(z, axis=1)\n",
    "    ag_litter_pool.columns = gcms\n",
    "    axes[2, 0].plot(ag_litter_pool, linewidth=0.7)\n",
    "    axes[2, 1].plot(ag_litter_pool.mean(axis=1))\n",
    "    axes[2, 0].title.set_text('agLitter Pool')\n",
    "\n",
    "    z = [x['bgLitter_Pool(gC/m2)_Delineated_Average'] for x in c_results_final[i]]\n",
    "    bg_litter_pool = pd.concat(z, axis=1)\n",
    "    bg_litter_pool.columns = gcms\n",
    "    axes[3, 0].plot(bg_litter_pool, linewidth=0.7) \n",
    "    axes[3, 1].plot(bg_litter_pool.mean(axis=1))\n",
    "    axes[3, 0].title.set_text('agLitter Pool')\n",
    "    \n",
    "    z = [x['Humus_Pool(gC/m2)_Delineated_Average'] for x in c_results_final[i]]\n",
    "    humus_pool = pd.concat(z, axis=1)\n",
    "    humus_pool.columns = gcms\n",
    "    axes[4, 0].plot(humus_pool, linewidth=0.7) \n",
    "    axes[4, 1].plot(humus_pool.mean(axis=1))\n",
    "    axes[4, 0].title.set_text('Humus Pool')\n",
    "    \n",
    "\n",
    "plt.ylabel('gC/m2')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "soil = np.loadtxt(config.velma_data / 'soil' / 'MapunitRaster_10m.asc', dtype='int', delimiter=',', skiprows=6)\n",
    "plt.figure()\n",
    "plt.imshow(soil)\n",
    "np.unique(soil)\n",
    "soil.dtype\n",
    "# soil_int = soil.astype('int')\n",
    "# soil_int\n",
    "\n",
    "from soil.soil_merger import readHeader\n",
    "header = readHeader(config.velma_data / 'soil' / 'MapunitRaster_10m.asc')\n",
    "outfile = config.velma_data / 'soil' / 'MapunitRaster_10m.asc'\n",
    "f = open(outfile, 'w')\n",
    "f.write(header)\n",
    "np.savetxt(f, soil_int.astype(int), fmt='%i')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnc_velma",
   "language": "python",
   "name": "tnc_velma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
