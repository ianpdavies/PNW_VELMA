{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing VELMA simulated runoff output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import __init__\n",
    "import scripts.config as config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import datetime\n",
    "from sklearn.svm import SVR\n",
    "import geopandas as gpd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "XSMALL_SIZE = 6\n",
    "SMALL_SIZE = 7\n",
    "MEDIUM_SIZE = 9\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams['figure.dpi'] = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and format observed data\n",
    "runoff_obs = pd.read_csv(config.velma_data / 'runoff' / 'ellsworth_Q_2004_2007.csv', names=['runoff_obs'])\n",
    "runoff_obs.index = pd.date_range('01-01-2004', '12-31-2007')\n",
    "runoff_obs['doy'], runoff_obs['year'] = runoff_obs.index.dayofyear, runoff_obs.index.year\n",
    "quality = pd.read_csv(str(config.streamflow), usecols=['Date', 'Quality'], parse_dates=True, index_col=0)\n",
    "quality = quality[(quality.index >= pd.to_datetime('01-01-2004')) & (quality.index <= pd.to_datetime('12-31-2007'))]\n",
    "\n",
    "precip = pd.read_csv(config.velma_data / 'precip' / 'ellsworth_ppt_2004_2019.csv', names=['precip'])\n",
    "precip.index = pd.date_range('01-01-2004', '12-31-2019')\n",
    "precip['doy'], precip['year'] = precip.index.dayofyear, precip.index.year\n",
    "\n",
    "temp = pd.read_csv(config.velma_data / 'temp' / 'ellsworth_temp_2004_2019.csv', names=['temp'])\n",
    "temp.index = pd.date_range('01-01-2004', '12-31-2019')\n",
    "temp['doy'], temp['year'] = temp.index.dayofyear, temp.index.year\n",
    "\n",
    "# Import VELMA outputs\n",
    "results_dir = config.data_path.parents[0] / 'results'\n",
    "velma_results = pd.read_csv(results_dir / 'ellsworth_baseline_04_07/DailyResults.csv')\n",
    "\n",
    "# Format datetime of results\n",
    "jday_pad = velma_results['Day'].apply(lambda x: str(x).zfill(3))\n",
    "str_year = velma_results['Year'].apply(lambda x: str(x))\n",
    "velma_results['year_jday'] = str_year + jday_pad\n",
    "velma_results.index = pd.to_datetime(velma_results['year_jday'], format='%Y%j')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MSE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "32.57802004304731"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'RMSE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.707715834118523"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'R2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5665312861952982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# velma_results['Runoff_All(mm/day)_Delineated_Average'].sum() / runoff_obs['runoff_obs'].sum()\n",
    "sim_mse = mean_squared_error(runoff_obs['runoff_obs'], velma_results['Runoff_All(mm/day)_Delineated_Average'])\n",
    "display('MSE', sim_mse)\n",
    "\n",
    "display('RMSE', np.sqrt(sim_mse))\n",
    "\n",
    "display('R2', r2_score(runoff_obs['runoff_obs'], velma_results['Runoff_All(mm/day)_Delineated_Average']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group measurements by year\n",
    "runoff_sim_yearly = pd.pivot_table(velma_results, index=['Day'], columns=['Year'],\n",
    "                                   values=['Runoff_All(mm/day)_Delineated_Average'])\n",
    "runoff_obs_yearly = pd.pivot_table(runoff_obs, index=['doy'], columns=['year'], values=['runoff_obs'])\n",
    "precip_yearly = pd.pivot_table(precip, index=['doy'], columns=['year'], values=['precip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2eaa611f555441e9828a9603aa0cfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Observed vs. VELMA simulated runoff\n",
    "years = runoff_obs_yearly.columns.get_level_values(1)\n",
    "fig, axes = plt.subplots(ncols=1, nrows=len(years), figsize=(6, 9))\n",
    "for col, year in enumerate(years):\n",
    "    runoff_obs_yearly.iloc[:, col].plot(ax=axes[col], label='Observed', linewidth=1)\n",
    "    runoff_sim_yearly.iloc[:, col].plot(ax=axes[col], label='Simulated', linewidth=1)\n",
    "    axes[col].set_title(year)\n",
    "    axes[col].set_ylim([0, 80])\n",
    "axes[0].legend(loc='upper left', bbox_to_anchor=(0, 1.3), fancybox=True, ncol=2)\n",
    "axes[0].set_ylabel('Runoff (mm/day)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd60643296a4bfe86252b1b23631fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Runoff and precip\n",
    "years = runoff_obs_yearly.columns.get_level_values(1)\n",
    "fig, axes = plt.subplots(ncols=1, nrows=len(years), figsize=(6, 9))\n",
    "\n",
    "for col, year in enumerate(years):\n",
    "    ax2 = axes[col].twinx()\n",
    "    precip_yearly.iloc[:, col].plot(ax=ax2, label='Precip', linewidth=0.5, color='tab:green')\n",
    "    if col == 0:\n",
    "        ax2.set_ylabel('Precipitation (mm/day)')\n",
    "        ax2.legend(loc='upper right', bbox_to_anchor=(1, 1.3), fancybox=True, ncol=1)\n",
    "    runoff_obs_yearly.iloc[:, col].plot(ax=axes[col], label='Observed', linewidth=1)\n",
    "    runoff_sim_yearly.iloc[:, col].plot(ax=axes[col], label='Simulated', linewidth=1)\n",
    "    ax2.invert_yaxis()\n",
    "    axes[col].set_ylim([0, 80])\n",
    "    axes[col].set_title(year)\n",
    "axes[0].legend(loc='upper left', bbox_to_anchor=(0, 1.3))\n",
    "axes[0].set_ylabel('Runoff (mm/day)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f62881d686d4e698e9eefdaea7d3b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add quality scores below plot\n",
    "\n",
    "colors = sns.color_palette('Dark2', 6)\n",
    "\n",
    "quality_edit = quality.copy()\n",
    "\n",
    "quality_edit['doy'], quality_edit['year'] = quality_edit.index.dayofyear, quality_edit.index.year\n",
    "quality_edit_yearly = pd.pivot_table(quality_edit, index=['doy'], columns=['year'], values=['Quality'])\n",
    "quality8 = quality_edit_yearly.replace([2, 3, 8, 10, 50, 77, 160, 161, 179, 254],\n",
    "                                       [np.nan, np.nan, -2, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "quality10 = quality_edit_yearly.replace([2, 3, 8, 10, 50, 77, 160, 161, 179, 254],\n",
    "                                        [np.nan, np.nan, np.nan, -2, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "quality160 = quality_edit_yearly.replace([2, 3, 8, 10, 50, 77, 160, 161, 179, 254],\n",
    "                                         [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, -2, np.nan, np.nan, np.nan])\n",
    "quality161 = quality_edit_yearly.replace([2, 3, 8, 10, 50, 77, 160, 161, 179, 254],\n",
    "                                         [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, -2, np.nan, np.nan])\n",
    "quality179 = quality_edit_yearly.replace([2, 3, 8, 10, 50, 77, 160, 161, 179, 254],\n",
    "                                         [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, -2, np.nan])\n",
    "quality254 = quality_edit_yearly.replace([2, 3, 8, 10, 50, 77, 160, 161, 179, 254],\n",
    "                                         [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, -2])\n",
    "\n",
    "years = runoff_obs_yearly.columns.get_level_values(1)\n",
    "fig, axes = plt.subplots(ncols=1, nrows=len(years), figsize=(6, 9))\n",
    "\n",
    "for col, year in enumerate(years):\n",
    "    ax2 = axes[col].twinx()\n",
    "    precip_yearly.iloc[:, col].plot(ax=ax2, label='Precip', color='tab:green', linewidth=0.5)\n",
    "    if col == 0:\n",
    "        ax2.set_ylabel('Precipitation (mm/day)')\n",
    "        ax2.legend(loc='upper right', bbox_to_anchor=(1, 1.3), fancybox=True, ncol=1)\n",
    "    quality8.iloc[:, col].plot(ax=axes[col], color=colors[0], linewidth=2, label='Below rating')\n",
    "    quality10.iloc[:, col].plot(ax=axes[col], color=colors[1], linewidth=2, label='Above rating (<2x)')\n",
    "    quality160.iloc[:, col].plot(ax=axes[col], color=colors[2], linewidth=2, label='Above rating (>2x)')\n",
    "    quality161.iloc[:, col].plot(ax=axes[col], color=colors[3], linewidth=2, label='Below rating (<1/2x)')\n",
    "    quality179.iloc[:, col].plot(ax=axes[col], color=colors[4], linewidth=2, label='Estimated - unreliable')\n",
    "    quality254.iloc[:, col].plot(ax=axes[col], color=colors[5], linewidth=2, label='Rating exceeded')\n",
    "    runoff_obs_yearly.iloc[:, col].plot(ax=axes[col], label='Observed', linewidth=1)\n",
    "    runoff_sim_yearly.iloc[:, col].plot(ax=axes[col], label='Simulated', linewidth=1)\n",
    "    ax2.invert_yaxis()\n",
    "    axes[col].set_ylim([-5, 80])\n",
    "    axes[col].set_title(year)\n",
    "axes[0].legend(loc='upper left', bbox_to_anchor=(0, 1.7), fancybox=True, ncol=2)\n",
    "axes[0].set_ylabel('Runoff (mm/day)')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('baseline_runoffs_qaflags.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily flow from 15-minute data and compare to simulated runoff\n",
    "\n",
    "runoff_obs_15min = pd.read_csv(config.streamflow.parents[0] / 'ells_streamflow_15min_2003_2008.csv',\n",
    "                               parse_dates={'date_full': ['date', 'time']},\n",
    "                               usecols=['flow_cfs', 'date', 'time'],\n",
    "                               dtype={'flow_cfs': np.float32, 'quality': np.int32})\n",
    "\n",
    "# Convert streamflow from cfs to mm/day\n",
    "# 2.446576 ft3/sec =  1m3/35.314667ft3 * 1/km2 * 86400sec/1day * 1km2/1000000m2 * 1000mm/1m\n",
    "ft3_sec = (1 / 35.314667) * 86400 * (1 / 1000000) * 1000\n",
    "area = 13.7393  # area of upstream Ellsworth watershed, sq. km\n",
    "runoff_obs_15min['flow_mm_day'] = (runoff_obs_15min['flow_cfs'] / area) * ft3_sec\n",
    "runoff_obs_15min.drop('flow_cfs', axis=1, inplace=True)\n",
    "\n",
    "runoff_obs_15min.set_index('date_full', drop=True, inplace=True)\n",
    "runoff_obs_15min['minute'] = runoff_obs_15min.index.minute\n",
    "runoff_obs_15min['hour'] = runoff_obs_15min.index.hour\n",
    "runoff_obs_15min['doy'] = runoff_obs_15min.index.dayofyear\n",
    "runoff_obs_15min['doy_min_hour'] = (\n",
    "            runoff_obs_15min['doy'].apply(lambda x: str(x)) + runoff_obs_15min['hour'].apply(lambda x: str(x)) +\n",
    "            runoff_obs_15min['minute'].apply(lambda x: str(x))).apply(lambda x: int(x))\n",
    "runoff_obs_15min['year'] = runoff_obs_15min.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882c4e3e229d47c790972121299e66fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot 15 minute runoff\n",
    "runoff_obs_15min_yearly = pd.pivot_table(runoff_obs_15min, index=['doy_min_hour'], columns=['year'],\n",
    "                                         values=['flow_mm_day'])\n",
    "\n",
    "# Plot average daily runoff\n",
    "daily_means = runoff_obs_15min.groupby(pd.Grouper(freq='1D')).mean()\n",
    "runoff_obs_15min_means_yearly = pd.pivot_table(runoff_obs_15min, index=['doy'], columns=['year'],\n",
    "                                               values=['flow_mm_day'])\n",
    "\n",
    "# Plot average daily runoff vs. VELMA simulated runoff\n",
    "runoff_obs_15min_means_yearly.drop(columns=[('flow_mm_day', 2003), ('flow_mm_day', 2008)], inplace=True)\n",
    "years = runoff_obs_15min_means_yearly.columns.get_level_values(1)\n",
    "fig, axes = plt.subplots(ncols=1, nrows=len(years), figsize=(6, 9))\n",
    "for col, year in enumerate(years):\n",
    "    runoff_obs_15min_means_yearly.iloc[:, col].plot(ax=axes[col], label='Observed (15min mean)', linewidth=1)\n",
    "    runoff_sim_yearly.iloc[:, col].plot(ax=axes[col], label='Simulated', linewidth=1)\n",
    "    axes[col].set_title(year)\n",
    "axes[0].legend(loc='upper left', bbox_to_anchor=(0, 1.3), fancybox=True, ncol=2)\n",
    "axes[0].set_ylabel('Runoff (mm/day)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40774519548670946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317fad9388bf4f6f8e9fd989d69c15f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use model for runoff imputation to estimate removed values\n",
    "\n",
    "flow_path = config.streamflow\n",
    "flow = pd.read_csv(str(flow_path), usecols=['Date', 'Flow_cfs'], parse_dates=True, index_col=0)\n",
    "\n",
    "# Convert streamflow from cfs to mm/day\n",
    "# 2.446576 ft3/sec =  1m3/35.314667ft3 * 1/km2 * 86400sec/1day * 1km2/1000000m2 * 1000mm/1m\n",
    "ft3_sec = (1 / 35.314667) * 86400 * (1 / 1000000) * 1000\n",
    "area = 13.7393  # area of upstream Ellsworth watershed, sq. km\n",
    "flow['flow_mm_day'] = (flow['Flow_cfs'] / area) * ft3_sec\n",
    "flow.drop('Flow_cfs', axis=1, inplace=True)\n",
    "\n",
    "# Expand date range to include every day of all the years present\n",
    "begin = '01-01-{}'.format(flow.index.to_frame()['Date'].min().year)\n",
    "end = '12-31-{}'.format(flow.index.to_frame()['Date'].max().year)\n",
    "rng = pd.date_range(begin, end)\n",
    "df = pd.DataFrame(index=rng)\n",
    "daily_flow = df.merge(flow, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Feature engineering\n",
    "precip = pd.read_csv(str(config.daily_ppt), parse_dates=True, index_col=0)\n",
    "temp_mean = pd.read_csv(str(config.daily_temp_mean), parse_dates=True, index_col=0)\n",
    "temp_mean_min = pd.read_csv(str(config.daily_temp_min), parse_dates=True, index_col=0)\n",
    "temp_mean_max = pd.read_csv(str(config.daily_temp_max), parse_dates=True, index_col=0)\n",
    "df = pd.concat([precip, temp_mean, temp_mean_min, temp_mean_max], axis=1)\n",
    "\n",
    "# Convert day of year to signal\n",
    "day = 24 * 60 * 60\n",
    "year = 365.2425 * day\n",
    "timestamp_secs = pd.to_datetime(df.index)\n",
    "timestamp_secs = timestamp_secs.map(datetime.datetime.timestamp)\n",
    "df['year_cos'] = np.cos(timestamp_secs * (2 * np.pi / year))\n",
    "df['year_sin'] = np.sin(timestamp_secs * (2 * np.pi / year))\n",
    "\n",
    "# Sum of last 2 days precip\n",
    "df['precip_sum-2t'] = precip.rolling(2).sum()\n",
    "\n",
    "# Previous days' precip\n",
    "df['precip_t-1'] = precip['mean_ppt_mm'].shift(1)\n",
    "df['precip_t-2'] = precip['mean_ppt_mm'].shift(2)\n",
    "df['precip_t-3'] = precip['mean_ppt_mm'].shift(3)\n",
    "\n",
    "obs = df.merge(daily_flow['flow_mm_day'], left_index=True, right_index=True, how='right')\n",
    "\n",
    "# Set aside dates with missing flow measurements\n",
    "gap_data = obs[obs['flow_mm_day'].isna()]\n",
    "obs.dropna(inplace=True)\n",
    "\n",
    "# Remove days with low quality measurements\n",
    "low_qual = quality[(quality['Quality'].isin([8, 10, 160, 161, 179, 254]))]\n",
    "shared_index = obs.index.intersection(low_qual.index)\n",
    "obs_highqual = obs.drop(shared_index)\n",
    "\n",
    "def plot_test_results(y_test, y_pred):\n",
    "    results = pd.DataFrame(data=np.column_stack([y_test, y_pred]), index=y_test.index, columns=['y_test', 'y_pred'])\n",
    "    results = (results * train_std['flow_mm_day']) + train_mean['flow_mm_day']\n",
    "    plt.plot(results, linewidth=0.8)\n",
    "    plt.legend(results.columns)\n",
    "\n",
    "\n",
    "# Split the data 70-20-10\n",
    "n = obs_highqual.shape[0]\n",
    "train_df = obs_highqual[0:int(n * 0.7)]\n",
    "test_df = obs_highqual[int(n * 0.7):]\n",
    "num_features = obs_highqual.shape[1]\n",
    "\n",
    "cols = obs_highqual.columns.tolist()\n",
    "target = cols.index('flow_mm_day')\n",
    "\n",
    "# Normalize\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "X_train, y_train = train_df.iloc[:, 0:target], train_df.iloc[:, target]\n",
    "X_test, y_test = test_df.iloc[:, 0:target], test_df.iloc[:, target]\n",
    "\n",
    "svr = SVR(kernel='rbf', C=1, gamma='auto', epsilon=0.1)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svr.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "plt.figure()\n",
    "plot_test_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db97f055cb64aad9fa856a88fadd5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predicting measurements removed for low quality\n",
    "\n",
    "velma_start = pd.to_datetime('01-01-2004')\n",
    "velma_end = pd.to_datetime('12-31-2007')\n",
    "\n",
    "# Remove dates with poor quality tags/rating table exceedances\n",
    "low_qual = quality[(quality['Quality'].isin([8, 10, 160, 161, 179, 254]))]\n",
    "gap_data = low_qual.merge(df, left_index=True, right_index=True, how='left')\n",
    "gap_data_04_07 = gap_data[(gap_data.index >= velma_start) & (gap_data.index <= velma_end)].copy()\n",
    "X_gap = gap_data_04_07.drop(columns=['Quality'], axis=1)\n",
    "\n",
    "X_gap = (X_gap - train_mean[:-1]) / train_std[:-1]\n",
    "gap_pred = svr.predict(X_gap)\n",
    "gap_pred = (gap_pred * train_std['flow_mm_day']) + train_mean['flow_mm_day']\n",
    "\n",
    "gap_data_04_07['flow_mm_day'] = gap_pred\n",
    "\n",
    "# Combine imputed flow with observed data\n",
    "gap_data_04_07_imp = gap_data_04_07.drop(columns=['Quality', 'mean_ppt_mm', 'mean_temp_c', 'Mean', 'Mean', 'year_cos',\n",
    "                                                  'year_sin', 'precip_sum-2t', 'precip_t-1', 'precip_t-2',\n",
    "                                                  'precip_t-3'], axis=1).copy()\n",
    "gap_data_04_07_imp['doy'], gap_data_04_07_imp['year'] = gap_data_04_07_imp.index.dayofyear, gap_data_04_07_imp.index.year\n",
    "\n",
    "runoff_obs_drop = runoff_obs.drop(gap_data_04_07_imp.index)\n",
    "runoff_obs_drop = runoff_obs_drop.rename(columns={'runoff_obs': 'flow_mm_day'})\n",
    "runoff_obs_imp = pd.concat([runoff_obs_drop, gap_data_04_07_imp]).sort_index()\n",
    "\n",
    "# Plot imputed data vs. VELMA simulated data\n",
    "runoff_obs_imp_yearly = pd.pivot_table(runoff_obs_imp, index=['doy'], columns=['year'], values=['flow_mm_day'])\n",
    "\n",
    "# Plot average daily runoff vs. VELMA simulated runoff\n",
    "years = runoff_obs_imp_yearly.columns.get_level_values(1)\n",
    "fig, axes = plt.subplots(ncols=1, nrows=len(years), figsize=(6, 9))\n",
    "for col, year in enumerate(years):\n",
    "    runoff_obs_imp_yearly.iloc[:, col].plot(ax=axes[col], label='Observed (imputed)', linewidth=1)\n",
    "    runoff_sim_yearly.iloc[:, col].plot(ax=axes[col], label='Simulated', linewidth=1)\n",
    "    axes[col].set_title(year)\n",
    "    axes[col].set_ylim([0, 80])\n",
    "axes[0].legend(loc='upper left', bbox_to_anchor=(0, 1.3), fancybox=True, ncol=2)\n",
    "axes[0].set_ylabel('Runoff (mm/day)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Nash-Sutcliffe\n",
    "def NS(s, o):\n",
    "    \"\"\"\n",
    "        Nash Sutcliffe efficiency coefficient\n",
    "        input:\n",
    "        s: simulated\n",
    "        o: observed\n",
    "        output:\n",
    "        ns: Nash Sutcliffe efficient coefficient\n",
    "        \"\"\"\n",
    "    # s,o = filter_nan(s,o)\n",
    "    return 1 - np.sum((s-o)**2)/np.sum((o-np.mean(o))**2)\n",
    "\n",
    "NS(velma_results['Runoff_All(mm/day)_Delineated_Average'], runoff_obs_imp['flow_mm_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NS(velma_results['Runoff_All(mm/day)_Delineated_Average'], runoff_obs['runoff_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing select (but not all) days with quality codes\n",
    "\n",
    "# Use model for runoff imputation to estimate removed values\n",
    "\n",
    "low_qual = quality[(quality['Quality'].isin([2, 254, 179, 161]))]\n",
    "\n",
    "flow_path = config.streamflow\n",
    "flow = pd.read_csv(str(flow_path), usecols=['Date', 'Flow_cfs'], parse_dates=True, index_col=0)\n",
    "\n",
    "# Convert streamflow from cfs to mm/day\n",
    "# 2.446576 ft3/sec =  1m3/35.314667ft3 * 1/km2 * 86400sec/1day * 1km2/1000000m2 * 1000mm/1m\n",
    "ft3_sec = (1 / 35.314667) * 86400 * (1 / 1000000) * 1000\n",
    "area = 13.7393  # area of upstream Ellsworth watershed, sq. km\n",
    "flow['flow_mm_day'] = (flow['Flow_cfs'] / area) * ft3_sec\n",
    "flow.drop('Flow_cfs', axis=1, inplace=True)\n",
    "\n",
    "# Expand date range to include every day of all the years present\n",
    "begin = '01-01-{}'.format(flow.index.to_frame()['Date'].min().year)\n",
    "end = '12-31-{}'.format(flow.index.to_frame()['Date'].max().year)\n",
    "rng = pd.date_range(begin, end)\n",
    "df = pd.DataFrame(index=rng)\n",
    "daily_flow = df.merge(flow, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Feature engineering\n",
    "precip = pd.read_csv(str(config.daily_ppt), parse_dates=True, index_col=0)\n",
    "temp_mean = pd.read_csv(str(config.daily_temp_mean), parse_dates=True, index_col=0)\n",
    "temp_mean_min = pd.read_csv(str(config.daily_temp_min), parse_dates=True, index_col=0)\n",
    "temp_mean_max = pd.read_csv(str(config.daily_temp_max), parse_dates=True, index_col=0)\n",
    "df = pd.concat([precip, temp_mean, temp_mean_min, temp_mean_max], axis=1)\n",
    "\n",
    "# Convert day of year to signal\n",
    "day = 24 * 60 * 60\n",
    "year = 365.2425 * day\n",
    "timestamp_secs = pd.to_datetime(df.index)\n",
    "timestamp_secs = timestamp_secs.map(datetime.datetime.timestamp)\n",
    "df['year_cos'] = np.cos(timestamp_secs * (2 * np.pi / year))\n",
    "df['year_sin'] = np.sin(timestamp_secs * (2 * np.pi / year))\n",
    "\n",
    "# Sum of last 2 days precip\n",
    "df['precip_sum-2t'] = precip.rolling(2).sum()\n",
    "\n",
    "# Previous days' precip\n",
    "df['precip_t-1'] = precip['mean_ppt_mm'].shift(1)\n",
    "df['precip_t-2'] = precip['mean_ppt_mm'].shift(2)\n",
    "df['precip_t-3'] = precip['mean_ppt_mm'].shift(3)\n",
    "\n",
    "obs = df.merge(daily_flow['flow_mm_day'], left_index=True, right_index=True, how='right')\n",
    "\n",
    "# Set aside dates with missing flow measurements\n",
    "gap_data = obs[obs['flow_mm_day'].isna()]\n",
    "obs.dropna(inplace=True)\n",
    "\n",
    "# Remove days with low quality measurements\n",
    "shared_index = obs.index.intersection(low_qual.index)\n",
    "obs_highqual = obs.drop(shared_index)\n",
    "\n",
    "def plot_test_results(y_test, y_pred):\n",
    "    results = pd.DataFrame(data=np.column_stack([y_test, y_pred]), index=y_test.index, columns=['y_test', 'y_pred'])\n",
    "    results = (results * train_std['flow_mm_day']) + train_mean['flow_mm_day']\n",
    "    plt.plot(results)\n",
    "    plt.legend(results.columns)\n",
    "\n",
    "\n",
    "# Split the data 70-20-10\n",
    "n = obs_highqual.shape[0]\n",
    "train_df = obs_highqual[0:int(n * 0.7)]\n",
    "test_df = obs_highqual[int(n * 0.7):]\n",
    "num_features = obs_highqual.shape[1]\n",
    "\n",
    "cols = obs_highqual.columns.tolist()\n",
    "target = cols.index('flow_mm_day')\n",
    "\n",
    "# Normalize\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "X_train, y_train = train_df.iloc[:, 0:target], train_df.iloc[:, target]\n",
    "X_test, y_test = test_df.iloc[:, 0:target], test_df.iloc[:, target]\n",
    "\n",
    "svr = SVR(kernel='rbf', C=1, gamma='auto', epsilon=0.1)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svr.predict(X_test)\n",
    "print(mse(y_test, y_pred))\n",
    "\n",
    "plt.figure()\n",
    "plot_test_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting measurements removed for low quality\n",
    "\n",
    "velma_start = pd.to_datetime('01-01-2004')\n",
    "velma_end = pd.to_datetime('12-31-2007')\n",
    "\n",
    "# Remove dates with poor quality tags/rating table exceedances\n",
    "gap_data = low_qual.merge(df, left_index=True, right_index=True, how='left')\n",
    "gap_data_04_07 = gap_data[(gap_data.index >= velma_start) & (gap_data.index <= velma_end)].copy()\n",
    "X_gap = gap_data_04_07.drop(columns=['Quality'], axis=1)\n",
    "\n",
    "X_gap = (X_gap - train_mean[:-1]) / train_std[:-1]\n",
    "gap_pred = svr.predict(X_gap)\n",
    "gap_pred = (gap_pred * train_std['flow_mm_day']) + train_mean['flow_mm_day']\n",
    "\n",
    "gap_data_04_07['flow_mm_day'] = gap_pred\n",
    "\n",
    "# Combine imputed flow with observed data\n",
    "gap_data_04_07_imp = gap_data_04_07.drop(columns=['Quality', 'mean_ppt_mm', 'mean_temp_c', 'Mean', 'Mean', 'year_cos',\n",
    "                                                  'year_sin', 'precip_sum-2t', 'precip_t-1', 'precip_t-2',\n",
    "                                                  'precip_t-3'], axis=1).copy()\n",
    "gap_data_04_07_imp['doy'], gap_data_04_07_imp['year'] = gap_data_04_07_imp.index.dayofyear, gap_data_04_07_imp.index.year\n",
    "\n",
    "runoff_obs_drop = runoff_obs.drop(gap_data_04_07_imp.index)\n",
    "runoff_obs_drop = runoff_obs_drop.rename(columns={'runoff_obs': 'flow_mm_day'})\n",
    "runoff_obs_imp = pd.concat([runoff_obs_drop, gap_data_04_07_imp]).sort_index()\n",
    "\n",
    "# Plot imputed data vs. VELMA simulated data\n",
    "runoff_obs_imp_yearly = pd.pivot_table(runoff_obs_imp, index=['doy'], columns=['year'], values=['flow_mm_day'])\n",
    "\n",
    "# Plot average daily runoff vs. VELMA simulated runoff\n",
    "years = runoff_obs_imp_yearly.columns.get_level_values(1)\n",
    "fig, axes = plt.subplots(ncols=1, nrows=len(years), figsize=(6, 9))\n",
    "for col, year in enumerate(years):\n",
    "    runoff_obs_imp_yearly.iloc[:, col].plot(ax=axes[col], label='Observed (imputed)', linewidth=1)\n",
    "    runoff_sim_yearly.iloc[:, col].plot(ax=axes[col], label='Simulated', linewidth=1)\n",
    "    axes[col].set_title(year)\n",
    "    axes[col].set_ylim([0, 80])\n",
    "axes[0].legend(loc='upper left', bbox_to_anchor=(0, 1.3), fancybox=True, ncol=2)\n",
    "axes[0].set_ylabel('Runoff (mm/day)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NS(velma_results['Runoff_All(mm/day)_Delineated_Average'], runoff_obs_imp['flow_mm_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like removing some of the days with quality tags (\"Below rating, less than 1/2x\", \"Estimated data - unreliable\", \"Rating table exceeded (data will not be reported)\", and ? code 2 which was not in the key) improves the NS score slightly. Those were only a few values, but suggests that VELMA is simulating better than the NS may report due to some unreliable streamflow events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnc_velma",
   "language": "python",
   "name": "tnc_velma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
